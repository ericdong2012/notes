## day01

### 1.爬虫

#### 1.1概念

```
网络爬虫就是模拟浏览器，发送请求，获取响应，按照一定规则自动获取网络信息的程序
```



#### 1.2作用

```
数据采集
软件测试 　　　虫师　　https://www.cnblogs.com/fnng/
12306抢票
网站上的投票
网络安全 
...

```



#### 1.3分类

根据被爬取网站的数量不同，可以分为：

- 通用爬虫，如 搜索引擎
- 聚焦爬虫，如12306抢票，或专门抓取某一个（某一类）网站数据

根据是否以获取数据为目的，可以分为：

- 功能性爬虫，给你喜欢的明星投票、点赞
- 数据增量爬虫，比如招聘信息

根据url地址和对应的页面内容是否改变，数据增量爬虫可以分为：

- 基于url地址变化、内容也随之变化的数据增量爬虫
- url地址不变、内容变化的数据增量爬虫



#### 1.4流程(***)



![](images\爬虫的工作流程.png)



```
获取一个url
向url发送请求，并获取响应（需要http协议）
如果从响应中提取url，则继续发送请求获取响应
如果从响应中提取数据，则将数据进行保存
```



### 2.http协议(***)

#### 2.1http和https

- HTTP：超文本传输协议，默认端口号是80
  - 超文本：是指超过文本，不仅限于文本；还包括图片、音频、视频等文件
  - 传输协议：是指使用共用约定的固定格式来传递转换成字符串的超文本内容
- HTTPS：HTTP + SSL(安全套接字层)，即带有安全套接字层的超本文传输协，默认端口号：443
  - SSL对传输的内容（超文本，也就是请求体或响应体）进行加密



#### 2.2请求头和响应头

> 访问 www.baidu.com , 查看请求头、响应头、状态码



##### 请求头

```
Content-Type             text/html    application/json
Host (主机和端口号)
Connection (链接类型)　　　keep-alive
Upgrade-Insecure-Requests (升级为HTTPS请求)  
*User-Agent (浏览器名称)
*Referer (页面跳转处)    防盗链(图片，视频)
*Cookie (Cookie)
Authorization(用于表示HTTP协议中需要认证资源的认证信息，如前边web课程中用于jwt认证)
```



##### 响应头

```
Set-Cookie （对方服务器设置cookie到用户浏览器的缓存）
```



##### 状态码

> **所有的状态码都不可信，一切以是否从抓包得到的响应中获取到数据为准**

```
200：成功
302：跳转，新的url在响应的Location头中给出
303：浏览器对于POST的响应进行重定向至新的url
307：浏览器对于GET的响应重定向至新的url
403：资源不可用；服务器理解客户的请求，但拒绝处理它（没有权限）
404：找不到该页面
500：服务器内部错误
502：网关错误
504：网关操作，nginx, openretry有问题(502已经与后端建立了连接，但超时；504与后端连接未建立，超时)
503：服务器由于维护或者负载过重未能应答，在响应中可能可能会携带Retry-After响应头；有可能是因为爬虫频繁访问url，使服务器忽视爬虫的请求，最终返回503响应状态码
```



#### 2.3浏览器请求流程

```
浏览器在拿到域名对应的ip后，先向地址栏中的url发起请求，并获取响应
在返回的响应内容（html）中，会带有css、js、图片等url地址，以及ajax代码，浏览器按照响应内容中的顺序依次发送其他的请求，并获取相应的响应
浏览器每获取一个响应就对展示出的结果进行添加（加载），js，css等内容会修改页面的内容，js也可以重新发送请求，获取响应
从获取第一个响应并在浏览器中展示，直到最终获取全部响应，并在展示的结果中添加内容或修改---这个过程叫做浏览器的渲染
```

浏览器：

​	发送请求， 渲染页面

爬虫：

​	发送请求，不渲染页面



**骨骼:**

​	**html静态文件**

**肌肉文件:**

​	**js/ajax　文件**

**皮肤:**

​	**css/font/图片**















## day02

### 1. requests(***)

```
# 简介
	该模块主要用于发送请求获取响应，该模块有很多的替代模块，比如说urllib模块，但是在工作中用的最多的还是requests模块，requests的代码简洁易懂，相对于臃肿的urllib模块，使用requests编写的爬虫代码将会更少，而且实现某一功能将会简单。因此建议大家掌握该模块的使用
	
# 文档   
http://docs.python-requests.org/zh_CN/latest/index.html

# 作用
发送http请求，获取响应数据

# 安装(先创建虚拟环境　python3)
pip install requests

```



#### 1.1 response

1. 基本使用

```python
# 1.2.2-response.content
import requests 

# 目标url
url = 'https://www.baidu.com' 

# 向目标url发送get请求
response = requests.get(url)

# 打印响应内容
# print(response.text)
print(response.content.decode()) # 注意这里！

```

2. response.text 和 response.content 的区别

- response.text()
  - 类型：str
  - 解码类型： requests模块自动根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码

- response.content.decode()  
  - 类型：bytes
  - 解码类型： 没有指定



3.中文乱码

- `response.content.decode()` 默认utf-8
- `response.content.decode("GBK")`
- 常见的编码字符集
  - utf-8
  - gbk
  - gb2312
  - ascii （读音：阿斯克码）
  - iso-8859-1



4.常用属性

- `response.url`响应的url；有时候响应的url和请求的url并不一致
- `response.status_code` 响应状态码
- `response.request.headers` 响应对应的请求头
- `response.headers` 响应头
- `response.request._cookies` 响应对应请求的cookie；返回cookieJar类型
- `response.cookies` 响应的cookie（经过了set-cookie动作；返回cookieJar类型
- `response.json()`自动将json字符串类型的响应内容转换为python对象（dict or list）

```python
# 1.2.3-response其它常用属性
import requests

# 目标url
url = 'https://www.baidu.com'

# 向目标url发送get请求
response = requests.get(url)

# 打印响应内容
# print(response.text)
# print(response.content.decode())             # 注意这里！
print(response.url)                            # 打印响应的url
print(response.status_code)                    # 打印响应的状态码
print(response.request.headers)                # 打印响应对象的请求头
print(response.headers)                        # 打印响应头
print(response.request._cookies)            # 打印请求携带的cookies
print(response.cookies)                        # 打印响应中携带的cookies
```



#### 1.2 get

```
# 1.2.1-简单的代码实现
import requests 

# 目标url
url = 'https://www.baidu.com' 

# 向目标url发送get请求
response = requests.get(url)

# 打印响应内容
print(response.text)
```



#### 1.3 headers

```python
import requests

url = 'https://www.baidu.com'

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

# 在请求头中带上User-Agent，模拟浏览器发送请求
response = requests.get(url, headers=headers) 

print(response.content.decode())

```



#### 1.4 传参

##### url传参

> 浏览器 network ---headers---query string params 

```python
import requests

url = 'https://www.baidu.com/s?wd=python'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/72.0.3626.121 Safari/537.36'
}

response = requests.get(url, headers=headers)

with open('baidu.html', 'wb')as f:
    f.write(response.content)
```



##### params传参

```python
import requests

url = 'https://www.baidu.com/s'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/72.0.3626.121 Safari/537.36'
}

data = {
    "wd": "python"
}

response = requests.get(url, headers=headers, params=data)

print(response.url)
with open('baidu1.html', 'wb')as f:
    f.write(response.content)
```





#### 1.5 cookie

##### headers中携带cookies

> 自己注册一个github账号，再测试 ,  当前代码中的cookie已失效

```python
# coding:utf-8
import requests

url = 'https://github.com/exile-morganna'

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/72.0.3626.121 Safari/537.36',
    'Cookie': '_ga=GA1.2.1190047373.1543731773; _'
              'octo=GH1.1.1199554731.1543731773; has_recent_activity=1; _gat=1; tz=Asia%2FShanghai; '
              'user_session=IsN0sqpV56zDyNOGBoUWHPRtiIe25zQ0y2cUCmBw0ubT7Zta; _'
              '_Host-user_session_same_site=IsN0sqpV56zDyNOGBoUWHPRtiIe25zQ0y2cUCmBw0ubT7Zta; logged_in=yes; '
              'dotcom_user=exile-morganna; _gh_sess=T0NFUGJlZm5tQmJNQW8rdjhZUUVySm1adnF2TkNNZkpKdW9ZQWlIZFhhZ2QxaEhmWFJNMWZ1enIxWFZOQ2tzbjUxMG1keUtteXoyUWdUVndBRjAyS2cyUGs2RFhnSjJCQk1Ia2ZkNlJIK0ZoRWhQY1VsOG1TcTRQTXJJaVdIQW14OFZid1Ixbnd5NllIMjBMUmpRdndNM0V6VWRKd05vVjNrRXY4blQxNVVsdkE1dlhnMUFWcjhiOVhObGFxZ0lLLS15TzB6Rmg4Q0wxTktOTnVJU1lMdmlBPT0%3D--778a38d546dee1db9e70f8a7f12c7f46ee6878b9'
}

response = requests.get(url, headers=headers)
print(response.url)

with open('github_with_cookies.html', 'wb')as f:
    f.write(response.content)

```



##### cookies传参

```python
import requests

url = 'https://github.com/USER_NAME'

# 构造请求头字典
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'
}
# 构造cookies字典
cookies_str = '从浏览器中copy过来的cookies字符串'

cookies_dict = {cookie.split('=')[0]:cookie.split('=')[-1] for cookie in cookies_str.split('; ')}

# 请求头参数字典中携带cookie字符串
resp = requests.get(url, headers=headers, cookies=cookies_dict)

print(resp.text)
```



##### cookieJar

```python
#coding:utf-8
import requests
#
# # url地址中的协议不能缺失
url = "http://www.baidu.com"

response = requests.get(url)

print(response.cookies)

cookie_dict = requests.utils.dict_from_cookiejar(response.cookies)
print(cookie_dict)

cookie_jar = requests.utils.cookiejar_from_dict(cookie_dict)
print(cookie_jar)
```



#### 1.6 timeout

```
import requests

url = 'https://twitter.com'

# timeout=3表示：发送请求后，3秒钟内返回响应，否则就抛出异常
response = requests.get(url, timeout=3)     
```





#### 1.7 代理

##### 作用

```
proxy代理参数通过指定代理ip，让代理ip对应的正向代理服务器转发我们发送的请求，那么我们首先来了解一下代理ip以及代理服务器
```



##### 正向代理

```
为浏览器或客户端（发送请求的一方）转发请求的，叫做正向代理
浏览器知道服务器的真实ip地址，例如VPN
```

##### 反向代理

```
为处理请求的服务器转发请求的，叫做反向代理
浏览器不知道服务器的真实地址，例如nginx
```



##### 分类



1. 根据代理ip的匿名程度，代理IP可以分为下面三类：

   - 透明代理(Transparent Proxy)：透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以查到你是谁。目标服务器接收到的请求头如下：

     ```
     REMOTE_ADDR = Proxy IP
     HTTP_VIA = Proxy IP
     HTTP_X_FORWARDED_FOR = Your IP
     ```

   - 匿名代理(Anonymous Proxy)：使用匿名代理，别人只能知道你用了代理，无法知道你是谁。目标服务器接收到的请求头如下：

     ```
     REMOTE_ADDR = proxy IP
     HTTP_VIA = proxy IP
     HTTP_X_FORWARDED_FOR = proxy IP
     ```

   - 高匿代理(Elite proxy或High Anonymity Proxy)：高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。**毫无疑问使用高匿代理效果最好**。目标服务器接收到的请求头如下：

     ```
     REMOTE_ADDR = Proxy IP
     HTTP_VIA = not determined
     HTTP_X_FORWARDED_FOR = not determined
     ```

2. 根据网站所使用的协议不同，需要使用相应协议的代理服务。从代理服务请求使用的协议可以分为：

   - http代理：目标url为http协议
   - https代理：目标url为https协议
   - socks隧道代理（例如socks5代理）等：
     1. socks 代理只是简单地传递数据包，不关心是何种应用协议（FTP、HTTP和HTTPS等）。
     2. socks 代理比http、https代理耗时少。
     3. socks 代理可以转发http和https的请求

##### 使用

> 快代理     https://www.kuaidaili.com/free/
>
> 西刺代理  https://www.xicidaili.com/
>
> 米扑代理  https://proxy.mimvp.com/
>
> 构建自己的代理池(向某一个代理网站发送请求，获取响应，分析响应，提取代理协议，ip， 端口，验证其是否可用，可用将其存到数据库中， 下次直接从数据库中取，看是否可用， 不可用从数据中删除；爬取多个代理网站，进而构建自己的代理池)

```python
#coding:utf-8
import requests

url = 'http://www.baidu.com'

# proxy = {
#     'http': 'http://101.231.104.82:80',
#     'https': 'https://101.231.104.82:80',
#     # 'https': 'https://1.192.246.63:9999',
# }


proxy = {
    'http': 'http://user:pwd@101.231.104.82:80',
    'https': 'https://user:pwd@101.231.104.82:80',
    # 'https': 'https://1.192.246.63:9999',
}

response = requests.get(url, proxies=proxy)
```



**注意proxy格式： proxy = { '协议’: '协议://ip:端口'}**



#### 1.8 CA

> verify参数能够忽略CA证书的认证

```
#coding:utf-8
import requests

url = 'https://sam.huat.edu.cn:8443/selfservice/'

response = requests.get(url, verify=False)

print(response.content)
```



#### 1.9 post

案例： 金山词霸(  http://fy.iciba.com/   )

```python
# coding:utf-8
import requests
import json
import sys


class King(object):
    def __init__(self, word):
        self.url = 'http://fy.iciba.com/ajax.php?a=fy'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36',
        }
        self.formdata = {
            'f': 'auto',
            't': 'auto',
            'w': word
        }

    def get_data(self):
        response = requests.post(self.url, headers=self.headers, data=self.formdata)
        print(response.content.decode())
        return response.content.decode()

    def parse_data(self, data):
        dict_data = json.loads(data)
        try:
            print(dict_data['content']['out'])
        except:
            print(dict_data['content']['word_mean'][0])

    def run(self):
        # 构思爬取思路
        # url
        # headers
        # formdata
        # 发送请求获取响应
        data = self.get_data()
        # print(data)

        # 解析响应
        self.parse_data(data)


if __name__ == '__main__':
    # king = King("人生苦短，及时行乐")
    # word = input('请输入:')
    word = sys.argv[1]
    king = King(word)
    king.run()

```



案例二:百度翻译(https://fanyi.baidu.com)

> 知道分析即可
>
> post 传参在　https://fanyi.baidu.com/v2transapi　中
>
> token 在　https://fanyi.baidu.com　的响应中



案例三: 有道翻译(http://fanyi.youdao.com/)

> 知道分析即可
>
> post 传参在　http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule　中
>
> 在无痕模式下，对比两次发送请求的传参
>
> ​	salt: 15624188583704
> ​	sign: c2b756da204e23268e832ff213009fa7
> ​	ts: 1562418858370



**post数据来源:**

```
1. 固定值
2. 输入值
3. 预设值---响应的静态文件中获取   百度翻译(fanyi.baidu.com)
4. 预设值---发送请求 
5. 在客户端生成(js生成)          有道翻译(http://fanyi.youdao.com/)
```





#### 1.10 session

- requests.session的作用

  - 自动处理cookie，即 **下一次请求会带上前一次的cookie**
- requests.session的应用场景

  - 自动处理连续的多次请求过程中产生的cookie


```
session = requests.session() # 实例化session对象
response = session.get(url, headers, ...)
response = session.post(url, data, ...)
```



案例： github模拟登陆

> authenticity_token  在login接口返回的静态文件中

> 正则表达式　https://www.jianshu.com/p/5295c5988b7f
>
> 贪婪和非贪婪　https://blog.csdn.net/m0_37852369/article/details/79101892　

```python
import requests
import re

# 构造请求头字典
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36',
}

# 实例化session对象
session = requests.session()

# 访问登陆页获取登陆请求所需参数
response = session.get('https://github.com/login', headers=headers)

# 使用正则获取登陆请求所需参数
token = re.findall('name="authenticity_token" value="(.*?)" />', response.text)[0] 

# 构造登陆请求参数字典
data = {
    'commit': 'Sign in', # 固定值
    'utf8': '✓', # 固定值
    'authenticity_token': token, # 该参数在登陆页的响应内容中
    'login': input('输入github账号：'),
    'password': input('输入github账号：')
}

# 发送登陆请求（无需关注本次请求的响应）
session.post('https://github.com/session', headers=headers, data=data)

# 打印需要登陆后才能访问的页面  
# 更改成自己的github 网址
response = session.get('https://github.com/1596930226', headers=headers)
print(response.text)
```









## day03

### 数据提取

#### 1.响应内容分类

结构化的响应内容

- json字符串
  - 可以使用re、json、jsonpath等模块来提取特定数据

- xml字符串
  - 可以使用re、lxml等模块来提取特定数据

非结构化的响应内容

- html字符串
  - 可以使用re、lxml等模块来提取特定数据



补充： xml

```xml
//xml是一种可扩展标记语言，样子和html很像，功能更专注于对传输和存储数据

<bookstore>
<book category="COOKING">
  <title lang="en">Everyday Italian</title> 
  <author>Giada De Laurentiis</author> 
  <year>2005</year> 
  <price>30.00</price> 
</book>
<book category="CHILDREN">
  <title lang="en">Harry Potter</title> 
  <author>J K. Rowling</author> 
  <year>2005</year> 
  <price>29.99</price> 
</book>
<book category="WEB">
  <title lang="en">Learning XML</title> 
  <author>Erik T. Ray</author> 
  <year>2003</year> 
  <price>39.95</price> 
</book>
</bookstore>
```



#### 2.数据解析方法

![](images\数据解析.png)



#### 3.jsonpath

##### 使用场景

```
如果有一个多层嵌套的复杂字典，想要根据key和下标来批量提取value，这是比较困难的。jsonpath模块就能解决这个痛点，可以按照key对python字典进行批量数据提取
```



##### 安装

```
pip install jsonpath
```



##### 基本使用

```
from jsonpath import jsonpath
json_dict = json.loads(json_temp)
ret = jsonpath(json_dict, 'jsonpath语法规则字符串')    # 结果是列表，获取结果需要索引
```



##### 语法规则

> **常用： $    .    ..**

![](images\jsonpath的方法.png)



##### 示例1:

```python
book_dict = { 
  "store": {
    "book": [ 
      { "category": "reference",
        "author": "Nigel Rees",
        "title": "Sayings of the Century",
        "price": 8.95
      },
      { "category": "fiction",
        "author": "Evelyn Waugh",
        "title": "Sword of Honour",
        "price": 12.99
      },
      { "category": "fiction",
        "author": "Herman Melville",
        "title": "Moby Dick",
        "isbn": "0-553-21311-3",
        "price": 8.99
      },
      { "category": "fiction",
        "author": "J. R. R. Tolkien",
        "title": "The Lord of the Rings",
        "isbn": "0-395-19395-8",
        "price": 22.99
      }
    ],
    "bicycle": {
      "color": "red",
      "price": 19.95
    }
  }
}

from jsonpath import jsonpath

print(jsonpath(book_dict, '$..author')) # 返回列表，如果取不到将返回False
```



##### 示例2：拉勾网城市JSON文件

>  <http://www.lagou.com/lbs/getAllCitySearchLabels.json> 

```python
#coding:utf-8
import requests
import jsonpath
import json

headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
}
response = requests.get('https://www.lagou.com/lbs/getAllCitySearchLabels.json', headers=headers)

dict_data = json.loads(response.content.decode())

print(jsonpath.jsonpath(dict_data,'$..name'))
```



#### 4.xpath

##### 基本认识

```
lxml模块可以利用XPath规则语法，来快速的定位HTML\XML 文档中特定元素以及获取节点信息（文本内容、属性值）

XPath (XML Path Language) 是一门在 HTML\XML 文档中查找信息的语言，可用来在 HTML\XML 文档中对元素和属性进行遍历

提取xml、html中的数据需要lxml模块和xpath语法配合使用
```



##### 插件安装

```
详见视频老师课件 
```



##### 语法(***)

> xpath语法：    <http://www.w3school.com.cn/xpath/index.asp



基础节点语法

| 表达式 | 描述                                                         |
| ------ | ------------------------------------------------------------ |
| /      | 从根节点选取、或者是元素和元素间的过渡。　　　　　　　　　绝对路径 |
| //     | 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 相对路径 |
| .      | 选取当前节点。　　　　　　当前节点                           |
| ..     | 选取当前节点的父节点。　　上层节点                           |
| @      | 选取属性。                                                   |
| text() | 选取开闭标签中的文本。                                       |

```
＃糗事百科　　https://www.qiushibaike.com/

选择所有的h2下的文本
//h2/text()

获取所有的a标签的href
//a/@href

获取html下的head下的title的文本   从开闭标签中取文本内容
/html/head/title/text()

获取html下的head下的link标签的href
/html/head/link/@href
```





节点修饰语法

| 路径表达式                          | 结果                                                         |
| ----------------------------------- | ------------------------------------------------------------ |
| //title[@lang="eng"]                | 选择lang属性值为eng的所有title元素                           |
| /bookstore/book[1]                  | 选取属于 bookstore 子元素的第一个 book 元素。                |
| /bookstore/book[last()]             | 选取属于 bookstore 子元素的最后一个 book 元素。              |
| /bookstore/book[last()-1]           | 选取属于 bookstore 子元素的倒数第二个 book 元素。            |
| /bookstore/book[position()>1]       | 选择bookstore下面的book元素，从第二个开始选择                |
| //book/title[text()='Harry Potter'] | 选择所有book下的title元素，仅仅选择文本为Harry Potter的title元素 |
| /bookstore/book[price>35.00]/title  | 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 |
| //span[contains(text(),"下一页")]   | 包含下一页的span  属性或者文本都可以    模糊匹配             |

```
所有的学科的名称
//div[@class="nav_txt"]//a[@class="a_gd"]

第一个学科的链接
//div[@class="nav_txt"]/ul/li[1]/a/@href

最后一个学科的链接
//div[@class="nav_txt"]/ul/li[last()]/a/@href
```





其他节点语法

| 通配符 | 描述                 |
| ------ | -------------------- |
| @*     | 匹配任何属性节点     |
| *      | 匹配任何元素节点。   |
| \|     | 匹配多个条件         |
| node() | 匹配任何类型的节点。 |

```
全部的标签
//*[contains(text(),"下一页")]

58同城不同页面
//h2/a  | //td/a
```





#### 5.lxml

##### 安装

```
pip install lxml
```



##### 使用1

```python
# coding:utf-8
from lxml import etree

text = ''' 
<div> 
    <ul> 
        <li class="item-1">
            <a href="link1.html">first item</a>
        </li> 
        <li class="item-1">
            <a href="link2.html">second item</a>
        </li> 
        <li class="item-inactive">
            <a href="link3.html">third item</a>
        </li> 
        <li class="item-1">
            <a href="link4.html">fourth item</a>
        </li> 
        <li class="item-0">
            <a href="link5.html">fifth item</a> 
    </ul> 
</div> '''

# 将html源码创建成element对象
html = etree.HTML(text)

# print(html)
# print(dir(html))

# print(html.xpath('//li[1]/a/text()'))
# print(html.xpath('//li[1]/a/@href'))

# text_list = html.xpath('//a/text()')
# link_list = html.xpath('//a/@href')
# print(text_list)
# print(link_list)
#
# for text,link in zip(text_list, link_list):
#     print(text,link)


# 先分类
el_list = html.xpath('//a')

for el in el_list:
    # print(el.xpath('//text()')[0], el.xpath('//@href')[0])
    print(el.xpath('./text()')[0], el.xpath('./@href')[0])
    # print(el.xpath('.//text()')[0], el.xpath('.//@href')[0])
    # print(el.xpath('text()')[0], el.xpath('@href')[0])


# etree.HTML()能够自动补全html缺失的标签
# print(etree.tostring(html))

```



##### 使用2

> 百度贴吧
>
> tips : 
>
> 在浏览器中找到具体的标签 ，右键单击copy---copy xpath
>
> 注意浏览器的ua 
>
> 排除广告的li标签

```python
# coding:utf-8
import requests
from lxml import etree


class Tieba(object):
    def __init__(self, name):
        self.name = name
        self.url = 'http://tieba.baidu.com/f?kw={}'.format(self.name)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
            # 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1) '
        }

    def get_data(self, url):
        response = requests.get(url, headers=self.headers)
        # with open("temp.html", "wb") as f:
        #     f.write(response.content)
        return response.content

    def parse_list_page(self, data):
        """
         解析贴吧帖子列表页面的响应
        :param data: 帖子列表页面的响应
        :return: 帖子标题和链接列表 与 下一页url
        """
        data = data.decode().replace("<!--", "").replace("-->", "")
        html = etree.HTML(data)
		
        # 注意规则
        el_list = html.xpath('//li[@class=" j_thread_list clearfix"]/div/div[2]/div[1]/div[1]/a')
        # print(len(el_list))
        data_list = []
        for el in el_list:
            temp = {}
            temp['title'] = el.xpath('./text()')[0]
            temp['link'] = 'http://tieba.baidu.com' + el.xpath('./@href')[0]
            data_list.append(temp)
        try:
            next_url = 'http:' + html.xpath('//*[contains(text(),"下一页>")]/@href')[0]
        except:
            next_url = None

        return data_list, next_url


    def run(self):
        # url
        # headers

        next_url = self.url
        # list_page_data = self.get_data(next_url)
        # data_list, next_url = self.parse_list_page(list_page_data)
        # print(data_list)

        while True:
            # 发送列表请求，获取响应
            list_page_data = self.get_data(next_url)

            # 解析列表页面的响应，提取帖子列表数据和下一页url
            data_list, next_url = self.parse_list_page(list_page_data)

            # 遍历帖子列表，获取每一个详细url
            for data in data_list:
                print(data)
            # 翻页&循环终止条件
            if next_url == None:
                break


if __name__ == '__main__':
    tieba = Tieba("传智播客")
    tieba.run()

```







## day04

### selenium

#### 1.运行效果

#### 2.工作原理

![](images/selenium的工作原理.png)





#### 3.安装

> 我们以谷歌浏览器的chromedriver为例
>
> chrome 与  chromedriver 对照表  https://npm.taobao.org/mirrors/chromedriver/2.46/notes.txt
>
>
>
> chrome(如果chrome 版本为50  ，ChromeDriver v2.22 (2016-06-06))
>
> https://npm.taobao.org/mirrors/chromedriver
>
>
>
> phantomjs
>
> https://npm.taobao.org/mirrors/phantomjs



3.1 在python虚拟环境中安装selenium模块

```
pip/pip3 install selenium
```



3.2 下载版本符合的webdriver

> 以chrome谷歌浏览器为例

1. 查看谷歌浏览器的版本

   ![查看chrome版本](images/查看chrome版本.png)

   ![查看chrome版本2](images/查看chrome版本2.png)



2.访问<https://npm.taobao.org/mirrors/chromedriver>

![下载chromedriver-1](images/下载chromedriver-1.png)



3.点击notes.txt进入版本说明页面

![下载chromedriver-2](images/下载chromedriver-2.png)



4.查看chrome和chromedriver匹配的版本

![下载chromedriver-3](images/下载chromedriver-3.png)

5.根据操作系统下载正确版本的chromedriver

![下载chromedriver-4](images/下载chromedriver-4.png)



6.解压压缩包后获取python代码可以调用的谷歌浏览器的webdriver可执行文件

- windows为`chromedriver.exe`
- linux和macos为`chromedriver`



7.chromedriver环境的配置(**可以不配置**)

- windows环境下需要将 chromedriver.exe 所在的目录设置为path环境变量中的路径
- linux/mac环境下，将 chromedriver 所在的目录设置到系统的PATH环境值中

```
# 将自己下载的chromedriver解压， 并复制到/usr/local/bin下
# 换成自己的chromedriver路径

sudo cp   /home/ive1025/chromedriver/chromedriver   /usr/local/bin
```





#### 4.简单使用

```python
import time
from selenium import webdriver

# 通过指定chromedriver的路径来实例化driver对象，chromedriver放在当前目录。
# driver = webdriver.Chrome(executable_path='./chromedriver')
# chromedriver已经添加环境变量
driver = webdriver.Chrome()

# 控制浏览器访问url地址
driver.get("https://www.baidu.com/")

# 在百度搜索框中搜索'python'
driver.find_element_by_id('kw').send_keys('python')
# 点击'百度搜索'
driver.find_element_by_id('su').click()

time.sleep(6)
# 退出浏览器
driver.quit()
```

- `webdriver.Chrome(executable_path='./chromedriver')`中executable参数指定的是下载好的chromedriver文件的路径

- `driver.find_element_by_id('kw').send_keys('python')`定位id属性值是'kw'的标签，并向其中输入字符串'python'

- ```
  driver.find_element_by_id('su').click()
  ```

  定位id属性值是su的标签，并点击

  - click函数作用是：触发标签的js的click事件





#### 5.属性和方法

1. `driver.page_source` 当前标签页浏览器渲染之后的网页源代码
2. `driver.current_url` 当前标签页的url
3. `driver.title`    当前标签的标题
4. `driver.close()` 关闭当前标签页，如果只有一个标签页则关闭整个浏览器
5. `driver.quit()` 关闭浏览器
6. `driver.forward()` 页面前进
7. `driver.back()` 页面后退
8. `driver.save_screenshot(img_name)` 页面截图



#### 6.元素定位

```python
find_element_by_id                      (返回一个元素)
find_element(s)_by_class_name           (根据类名获取元素列表)
find_element(s)_by_name                 (根据标签的name属性值返回包含标签对象元素的列表)
find_element(s)_by_xpath                (返回一个包含元素的列表)
find_element(s)_by_link_text            (根据连接文本获取元素列表)
find_element(s)_by_partial_link_text    (根据链接包含的文本获取元素列表)
find_element(s)_by_tag_name             (根据标签名获取元素列表)
find_element(s)_by_css_selector         (根据css选择器来获取元素列表)
```

- 注意：

  - find_element和find_elements的区别：

    - 多了个s就返回列表，没有s就返回匹配到的第一个标签对象
    - find_element匹配不到就抛出异常，find_elements匹配不到就返回空列表

  - by_link_text和by_partial_link_tex的区别：全部文本和包含某个文本



#### 7.元素操作

- 对元素执行点击操作`element.click()`
  - 对定位到的标签对象进行点击操作
- 向输入框输入数据`element.send_keys(data)`
  - 对定位到的标签对象输入数据
- 获取文本`element.text`
  - 通过定位获取的标签对象的`text`属性，获取文本内容
- 获取属性值`element.get_attribute("属性名")`
  - 通过定位获取的标签对象的`get_attribute`函数，传入属性名，来获取属性的值



- 代码实现，如下：

  ```python
  from selenium import webdriver
  
  driver = webdriver.Chrome()
  
  driver.get('http://www.itcast.cn/')
  
  ret = driver.find_elements_by_tag_name('h2')
  print(ret[0].text) # 
  
  ret = driver.find_elements_by_link_text('黑马程序员')
  print(ret[0].get_attribute('href'))
  
  driver.quit()
  ```



#### 8.标签页切换

- 具体的方法

  ```
  # 1. 获取当前所有的标签页的句柄构成的列表
  current_windows = driver.window_handles
  
  # 2. 根据标签页句柄列表索引下标进行切换
  driver.switch_to.window(current_windows[0])
  ```

- 参考代码示例：

  ```python
  import time
  from selenium import webdriver
  
  driver = webdriver.Chrome()
  driver.get("https://www.baidu.com/")
  
  time.sleep(1)
  driver.find_element_by_id('kw').send_keys('python')
  time.sleep(1)
  driver.find_element_by_id('su').click()
  time.sleep(1)
  
  # 通过执行js来新开一个标签页
  js = 'window.open("https://www.sogou.com");'
  driver.execute_script(js)
  time.sleep(1)
  
  # 1. 获取当前所有的窗口
  windows = driver.window_handles
  
  time.sleep(2)
  # 2. 根据窗口索引进行切换
  driver.switch_to.window(windows[0])
  
  time.sleep(2)
  driver.switch_to.window(windows[1])
  
  time.sleep(3)
  driver.quit()
  ```



#### 9.iframe标签切换

> ##### iframe是html中常用的一种技术，即一个页面中嵌套了另一个网页，    selenium默认是访问不了frame中的内容的，对应的解决思路是`driver.switch_to.frame(frame_element)`。接下来我们通过qq邮箱模拟登陆来学习这个知识点

- 参考代码：

  ```python
  import time
  from selenium import webdriver
  
  driver = webdriver.Chrome()
  
  url = 'https://mail.qq.com/cgi-bin/loginpage'
  driver.get(url)
  time.sleep(2)
  
  login_frame = driver.find_element_by_id('login_frame') # 根据id定位 frame元素
  driver.switch_to.frame(login_frame) # 转向到该frame中
  
  driver.find_element_by_xpath('//*[@id="u"]').send_keys('1596930226@qq.com')
  time.sleep(2)
  
  driver.find_element_by_xpath('//*[@id="p"]').send_keys('hahamimashicuode')
  time.sleep(2)
  
  driver.find_element_by_xpath('//*[@id="login_button"]').click()
  time.sleep(2)
  
  """操作frame外边的元素需要切换出去"""
  windows = driver.window_handles
  driver.switch_to.window(windows[0])
  
  content = driver.find_element_by_class_name('login_pictures_title').text
  print(content)
  
  driver.quit()
  ```

总结：

- 切换到定位的frame标签嵌套的页面中

  - `driver.switch_to.frame(通过find_element_by函数定位的frame、iframe标签对象)`

- 利用切换标签页的方式切出frame标签

  - ```
    windows = driver.window_handles
    driver.switch_to.window(windows[0])
    ```



#### 10.cookie的处理

 获取cookie

> `driver.get_cookies()`返回列表，其中包含的是完整的cookie信息！不光有name、value，还有domain等cookie其他维度的信息。所以如果想要把获取的cookie信息和requests模块配合使用的话，需要转换为name、value作为键值对的cookie字典

```
# 获取当前标签页的全部cookie信息
print(driver.get_cookies())
# 把cookie转化为字典
cookies_dict = {cookie['name']: cookie['value'] for cookie in driver.get_cookies()}
```

删除cookie

```
#删除一条cookie
driver.delete_cookie("CookieName")

# 删除所有的cookie
driver.delete_all_cookies()
```



#### 11.执行js代码

```
import time
from selenium import webdriver

driver = webdriver.Chrome()
driver.get("http://www.itcast.cn/")
time.sleep(1)

js = 'window.scrollTo(0,document.body.scrollHeight)' # js语句  滚动到底部
driver.execute_script(js) # 执行js的方法

time.sleep(5)
driver.quit()
```



#### 12.页面等待

强制等待（了解）

- 其实就是time.sleep()
- 缺点时不智能，设置的时间太短，元素还没有加载出来；设置的时间太长，则会浪费时间



隐式等待

- 隐式等待针对的是元素定位，隐式等待设置了一个时间，在一段时间内判断元素是否定位成功，如果完成了，就进行下一步

- 在设置的时间内没有定位成功，则会报超时加载

- 示例代码

  ```
  from selenium import webdriver
  
  driver = webdriver.Chrome()  
  
  driver.implicitly_wait(10) # 隐式等待，最长等20秒  
  
  driver.get('https://www.baidu.com')
  
  driver.find_element_by_xpath()
  ```





#### 13.配置

> 配置参考　　https://www.jishuwen.com/d/2yxH



##### 无界面模式

- 开启无界面模式的方法
  - 实例化配置对象
    - `options = webdriver.ChromeOptions()`
  - 配置对象添加开启无界面模式的命令
    - `options.add_argument("--headless")`
  - 配置对象添加禁用gpu的命令
    - `options.add_argument("--disable-gpu")`
  - 实例化带有配置对象的driver对象
    - `driver = webdriver.Chrome(chrome_options=options)`
- 注意：macos中chrome浏览器59+版本，Linux中57+版本才能使用无界面模式！
- 参考代码如下：

```python
from selenium import webdriver

options = webdriver.ChromeOptions() # 创建一个配置对象
options.add_argument("--headless") # 开启无界面模式
options.add_argument("--disable-gpu") # 禁用gpu

# options.set_headles() # 无界面模式的另外一种开启方式
driver = webdriver.Chrome(chrome_options=options) # 实例化带有配置的driver对象

driver.get('http://www.itcast.cn')
print(driver.title)
driver.quit()
```



##### 代理ip

- 使用代理ip的方法

  - 实例化配置对象
    - `options = webdriver.ChromeOptions()`
  - 配置对象添加使用代理ip的命令
    - `options.add_argument('--proxy-server=http://202.20.16.82:9527')`
  - 实例化带有配置对象的driver对象
    - `driver = webdriver.Chrome('./chromedriver', chrome_options=options)`

- 参考代码如下：

  ```python
  from selenium import webdriver
  
  options = webdriver.ChromeOptions() # 创建一个配置对象
  options.add_argument('--proxy-server=http://202.20.16.82:9527') # 使用代理ip
  
  driver = webdriver.Chrome(chrome_options=options) # 实例化带有配置的driver对象
  
  driver.get('http://www.itcast.cn')
  print(driver.title)
  driver.quit()
  ```



##### user-agent

- 替换user-agent的方法

  - 实例化配置对象
    - `options = webdriver.ChromeOptions()`
  - 配置对象添加替换UA的命令
    - `options.add_argument('--user-agent=Mozilla/5.0 HAHA')`
  - 实例化带有配置对象的driver对象
    - `driver = webdriver.Chrome('./chromedriver', chrome_options=options)`

- 参考代码如下：

  ```python
  from selenium import webdriver
  
  options = webdriver.ChromeOptions() # 创建一个配置对象
  options.add_argument('--user-agent=Mozilla/5.0 HAHA') # 替换User-Agent
  
  driver = webdriver.Chrome('./chromedriver', chrome_options=options)
  
  driver.get('http://www.itcast.cn')
  print(driver.title)
  driver.quit()
  ```





## day05

### 1.常见的反爬和反反爬

反爬的三个方向

- 基于身份识别进行反爬
- 基于爬虫行为进行反爬
- 基于数据加密进行反爬



#### 基于身份识别进行反爬

**1 通过headers字段来反爬**

> headers中有很多字段，这些字段都有可能会被对方服务器拿过来进行判断是否为爬虫

1.1 通过headers中的User-Agent字段来反爬

- 反爬原理：爬虫默认情况下没有User-Agent，而是使用模块默认设置
- 解决方法：请求之前添加User-Agent即可；更好的方式是使用User-Agent池来解决（收集一堆User-Agent的方式，或者是随机生成User-Agent）

1.2 通过referer字段或者是其他字段来反爬

- 反爬原理：爬虫默认情况下不会带上referer字段，服务器端通过判断请求发起的源头，以此判断请求是否合法
- 解决方法：添加referer字段

1.3 通过cookie来反爬

- 反爬原因：通过检查cookies来查看发起请求的用户是否具备相应权限，以此来进行反爬
- 解决方案：进行模拟登陆，成功获取cookies之后在进行数据爬取

**2 通过请求参数来反爬**

> 请求参数的获取方法有很多，向服务器发送请求，很多时候需要携带请求参数，通常服务器端可以通过检查请求参数是否正确来判断是否为爬虫

2.1 通过从html静态文件中获取请求数据(github登录数据)

- 反爬原因：通过增加获取请求参数的难度进行反爬
- 解决方案：仔细分析抓包得到的每一个包，搞清楚请求之间的联系

2.2 通过发送请求获取请求数据

- 反爬原因：通过增加获取请求参数的难度进行反爬
- 解决方案：仔细分析抓包得到的每一个包，搞清楚请求之间的联系，搞清楚请求参数的来源

2.3 通过js生成请求参数

- 反爬原理：js生成了请求参数
- 解决方法：分析js，观察加密的实现过程，通过js2py获取js的执行结果，或者使用selenium来实现

2.4 通过验证码来反爬

- 反爬原理：对方服务器通过弹出验证码强制验证用户浏览行为
- 解决方法：打码平台或者是机器学习的方法识别验证码，其中打码平台廉价易用，更值得推荐



#### 基于爬虫行为进行反爬

**1 基于请求频率或总请求数量**

> 爬虫的行为与普通用户有着明显的区别，爬虫的请求频率与请求次数要远高于普通用户

1.1 通过请求ip/账号**单位时间内总请求数量**进行反爬

- 反爬原理：正常浏览器请求网站，速度不会太快，同一个ip/账号大量请求了对方服务器，有更大的可能性会被识别为爬虫
- 解决方法：对应的通过购买高质量的ip的方式能够解决问题/购买个多账号

1.2 通过同一ip/账号请求之间的**间隔**进行反爬

- 反爬原理：正常人操作浏览器浏览网站，请求之间的时间间隔是随机的，而爬虫前后两个请求之间时间间隔通常比较固定同时时间间隔较短，因此可以用来做反爬
- 解决方法：请求之间进行随机等待，模拟真实用户操作，在添加时间间隔后，为了能够高速获取数据，尽量使用代理池，如果是账号，则将账号请求之间设置随机休眠

1.3 通过对请求ip/账号**每天请求次数设置阈值**进行反爬

- 反爬原理：正常的浏览行为，其一天的请求次数是有限的，通常超过某一个值，服务器就会拒绝响应
- 解决方法：对应的通过购买高质量的ip的方法/多账号，同时设置请求间随机休眠



**2 根据爬取行为进行反爬，通常在爬取步骤上做分析**

2.1 通过js实现跳转来反爬

- 反爬原理：js实现页面跳转，无法在源码中获取下一页url
- 解决方法: 多次抓包获取条状url，分析规律

2.2 通过蜜罐(陷阱)获取爬虫ip(或者代理ip)，进行反爬

- 反爬原理：在爬虫获取链接进行请求的过程中，爬虫会根据正则，xpath，css等方式进行后续链接的提取，此时服务器端可以设置一个陷阱url，会被提取规则获取，但是正常用户无法获取，这样就能有效的区分爬虫和正常用户
- 解决方法: 完成爬虫的编写之后，使用代理批量爬取测试/仔细分析响应内容结构，找出页面中存在的陷阱

2.3 通过假数据反爬

- 反爬原理：向返回的响应中添加假数据污染数据库，通常家属剧不会被正常用户看到
- 解决方法: 长期运行，核对数据库中数据同实际页面中数据对应情况，如果存在问题/仔细分析响应内容

2.4 阻塞任务队列

- 反爬原理：通过生成大量垃圾url，从而阻塞任务队列，降低爬虫的实际工作效率
- 解决方法: 观察运行过程中请求响应状态/仔细分析源码获取垃圾url生成规则，对URL进行过滤

2.5 阻塞网络IO

- 反爬原理：发送请求获取响应的过程实际上就是下载的过程，在任务队列中混入一个大文件的url，当爬虫在进行该请求时将会占用网络io，如果是有多线程则会占用线程
- 解决方法: 观察爬虫运行状态/多线程对请求线程计时/发送请求钱

2.6 运维平台综合审计

- 反爬原理：通过运维平台进行综合管理，通常采用复合型反爬虫策略，多种手段同时使用

- #### 解决方法: 仔细观察分析，长期运行测试目标网站，检查数据采集速度，多方面处理





#### 基于数据加密进行反爬

**1 对响应中含有的数据进行特殊化处理**

> 通常的特殊化处理主要指的就是css数据偏移/自定义字体/数据加密/数据图片/特殊编码格式等

1.1 通过自定义字体来反爬 下图来自猫眼电影电脑版

![img](images/使用字体来反爬.png)

- 反爬思路: 使用自有字体文件
- 解决思路：切换到手机版/解析字体文件进行翻译

**1.2 通过css来反爬 下图来自  去哪儿电脑版**

![img](images/通过css反爬.png)

- 反爬思路：源码数据不为真正数据，需要通过css位移才能产生真正数据
- 解决思路：计算css的偏移

------

**1.3 通过js动态生成数据进行反爬**

- 反爬原理：通过js动态生成
- 解决思路：解析关键js，获得数据生成流程，模拟生成数据

**1.4 通过数据图片化反爬**

- 58同城短租](<https://baise.58.com/duanzu/38018718834984x.shtml>)
- 解决思路：通过使用图片解析引擎从图片中解析数据

**1.5 通过编码格式进行反爬**

- 反爬原理: 不适用默认编码格式，在获取响应之后通常爬虫使用utf-8格式进行解码，此时解码结果将会是乱码或者报错
- 解决思路：根据源码进行多格式解码，或者真正的解码格式





### 2.验证码

#### 安装

1 引擎的安装

- mac环境下直接执行命令

  ```
  brew install --with-training-tools tesseract
  ```

- windows环境下的安装 可以通过exe安装包安装，下载地址可以从GitHub项目中的wiki找到。安装完成后记得将Tesseract 执行文件的目录加入到PATH中，方便后续调用。

- linux环境下的安装

  ```
  sudo apt-get install tesseract-ocr
  ```



2 Python库的安装

```
# PIL用于打开图片文件
pip/pip3 install pillow

# pytesseract模块用于从图片中解析数据
pip/pip3 install pytesseract
```



#### 使用

**方式一:**

```
tesseract xxx result
```



**方式二：**

```
from PIL import Image
import pytesseract

im = Image.open()

result = pytesseract.image_to_string(im)

print(result)
```



#### 打码平台

##### 常见的打码平台

1. 云打码：<http://www.yundama.com/>

   能够解决通用的验证码识别

2. 极验验证码智能识别辅助：<http://jiyandoc.c2567.com/>

   能够解决复杂验证码的识别

##### 基本使用

> 见代码
>



### 3.js解析(***)

> www.renren.com

#### 定位js文件

> 弄清楚 rkey , password 的来源

方式一：

​	通过initiator 判断

方式二：

​	通过search in file 

方式三：

​	通过前端element 代码中看调用了哪个js

方式四：

​	通过前端element 行为，比如点击，看调用了哪个js



#### 分析js代码



#### js2py

js的执行方式大致分为两种：

1. 在了解了js内容和执行顺序之后，通过python来完成js的执行过程，得到结果（python 重现）
2. 在了解了js内容和执行顺序之后，使用类似js2py的模块来执js代码，得到结果  js2py ,pyv8, execjs

##### 示例：

```python
import js2py
import requests

context = js2py.EvalJs()
headers = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Mobile Safari/537.36"
}
big_js = requests.get("http://s.xnimg.cn/a85738/wap/mobile/wechatLive/js/RSA.js", headers=headers)

context.execute(big_js.content.decode())

print(context)
```



##### 案例：人人网登录



操作流程:

1.打开无痕浏览器，

2.打开浏览器调试窗口，勾选preserver log, 切换到手机端

3.输入

http://activity.renren.com/livecell/log?c1=-100&c2=&c3=&c4=&from_uid=&isFull=&QRCodeRecharge=



4.分析rkey , clog 两个接口



**具体代码**

```python
#!/usr/bin/env python
# -*- coding:utf-8 -*-

import requests
import json
import js2py

# 1. 获取session对象
session = requests.session()
headers = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 5.0; SM-G900P Build/LRX21T) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Mobile Safari/537.36",
    "X-Requested-With": "XMLHttpRequest",
    "Content-Type": "application/x-www-form-urlencoded"
}

# 2.设置session的请求头信息
session.headers = headers

# 3.发送获取公钥包的请求
response = session.get("http://activity.renren.com/livecell/rKey")
# print(response.content.decode())

# 4.创建n
n = json.loads(response.content.decode())['data']

# 5.创建t
t = {'password': "Abc123456"}

# 6.获取前置js
rsa_js = session.get("http://s.xnimg.cn/a85738/wap/mobile/wechatLive/js/RSA.js").content.decode()
barrett_js = session.get("http://s.xnimg.cn/a85738/wap/mobile/wechatLive/js/Barrett.js").content.decode()
bigint_js = session.get("http://s.xnimg.cn/a85738/wap/mobile/wechatLive/js/BigInt.js").content.decode()

# 7.创建js环境
context = js2py.EvalJs()

# 8.将变量加载到js环境中执行
context.execute(rsa_js)
context.execute(barrett_js)
context.execute(bigint_js)
context.n = n
context.t = t

# 执行加密密码的js字符
pwd_js = '''
       t.password = t.password.split("").reverse().join(""),
       setMaxDigits(130);
       var o = new RSAKeyPair(n.e,"",n.n)
        , r = encryptedString(o, t.password);
      '''
context.execute(pwd_js)
# - 通过context获取加密后密码信息
# print(context.r)

#   - 使用session发送登录请求
#     - URL: http://activity.renren.com/livecell/ajax/clog
#     - 请求方法: POST
#     - 数据:
#       - phoneNum: 15565280933
#       - password: (加密后生产的)
#       - c1: 0
#       - rKey: rkey请求获取的

formdata = {
    'phoneNum': '18171081846',
    'password': context.r,
    'c1': "-100",
    'rKey': n['rkey']
}

# print(session.headers)
response = session.post("http://activity.renren.com/livecell/ajax/clog", data=formdata)
print(response.content.decode())

# # 访问登录的资源
# response = session.get("http://activity.renren.com/home#profile")
# print(response.content.decode())

```



##### 案例：有道翻译

> fanyi.min.js --- smartresult --- generateSaltSign---r(function) 

hashlib的使用：

```
#!/usr/bin/env python
# -*- coding:utf-8 -*-

import hashlib

data = "python"

m = hashlib.md5()

m.update(data.encode())

result = m.hexdigest()

print(result)
```



去重：

url

url-hash

布隆过滤器



代码：

```python
import random

import requests
import hashlib
import time


class Youdao(object):
    def __init__(self, word):
        self.url = "http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Mobile Safari/537.36",
            "Referer": "http://fanyi.youdao.com/",
            "Cookie": "OUTFOX_SEARCH_USER_ID=1169614668@10.169.0.83; JSESSIONID=aaaAluUN9wIGe-O17R6Tw; OUTFOX_SEARCH_USER_ID_NCOO=65675660.48316651; YOUDAO_MOBILE_ACCESS_TYPE=0; ___rl__test__cookies=1561131564476"

        }

        self.formdata = None
        self.word = word

    def generate_formdata(self):
        """
            var r = function(e) {
                var t = n.md5(navigator.appVersion)
                  , r = "" + (new Date).getTime()
                  , i = r + parseInt(10 * Math.random(), 10);
                return {
                    ts:r,
                    bv:t,
                    salt:i,
                    sign:n.md5("fanyideskweb" + e + i + "@6f#X3=cCuncYssPsuRUE")
                }
            };

        """

        ts = str(int(time.time() * 1000))
        salt = ts + str(random.randint(0, 9))
        # print(ts)
        # print(salt)
        tempstr = "fanyideskweb" + self.word + salt + "@6f#X3=cCuncYssPsuRUE"
        md5 = hashlib.md5()
        md5.update(tempstr.encode())
        sign = md5.hexdigest()

        self.formdata = {
            "i": self.word,
            "from": "AUTO",
            "to": "AUTO",
            "smartresult": "dict",
            "client": "fanyideskweb",
            "salt": salt,
            "sign": sign,
            "ts": ts,
            "bv": "e31381909585c264b48834d1292eef54",
            "doctype": "json",
            "version": "2.1",
            "keyfrom": "fanyi.web",
            "action": "FY_BY_REALTlME"
        }

    def get_data(self):
        resp = requests.post(self.url, data=self.formdata, headers=self.headers)
        print(resp.content.decode())

    def run(self):
        self.generate_formdata()
        print(self.formdata)
        self.get_data()


if __name__ == '__main__':
    youdao = Youdao("人生苦短,及时行乐")
    youdao.run()
```





## day06

### mongodb

#### 1.安装

> 大家虚拟机中已经有mongodb , 可直接运行 mongo测试

1.1 命令安装

在ubuntu中使用apt-get工具安装

```
sudo apt-get install -y mongodb-org
```

> 或参考官方文档 <https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/>



1.2 源码安装

1.2.1 选择相应版本和操作系统并下载

> <https://www.mongodb.com/download-center/community?jmp=docs>



1.2.2 解压

> tar -zxvf mongodb-linux-x86_64-ubuntu1804-4.0.3.tgz



1.2.3 移动到/usr/local/目录下

> sudo mv -r mongodb-linux-x86_64-ubuntu1804-4.0.3/ /usr/local/mongodb



1.2.4 在shell的初始化脚本.bashrc中添加mongodb可执行文件到环境变量PATH中

> a. 进入.bashrc文件中

```
cd ~
sudo vi .bashrc
```

> b. 在.bashrc文件的最后添加：

```
export PATH=/usr/local/mongodb/bin:$PATH
```



#### 2.启动



##### mongodb服务端启动

- 默认端口：27017
- 默认配置文件的位置：/etc/mongod.conf
- 默认日志的位置：/var/log/mongodb/mongod.log



1.1 测试方式启动

- 启动: sudo service mongod start (sudo service mongod start)

- 停止: sudo service mongod stop

- 重启: sudo service mongod restart


1.2 生产环境正式的启动方式

> 启动: sudo mongod [--auth --dbpath=dbpath --logpath=logpath --append --fork] [-–f logfile ]

- 只以 sudo mongod 命令启动时，默认将数据存放在了 /data/db 目录下，需要手动创建
- --dbpath: 指定数据库的存放路径
- --logpath: 指定日志的存放路径
- --append: 或--logappend 设置日志的写入形式为追加模式
- --fork: 或-fork 开启新的进程运行mongodb服务
- --f: 或-f 配置文件路径(可以将上述配置信息写入文件然后通过该文件中的参数进行加载启动)
- --auth: 以权限认证的方式启动，我们会在后边的课程中学习该内容



1.3 查看是否启动成功

> ##### ps aux | grep mongod



##### mongodb客户端启动

- 启动本地客户端: mongo

- 查看帮助：mongo –help

- ##### 退出：exit或者ctrl+c



#### 3.命令使用

> 不知道输入什么指令，可以直接输入 help, 或则tab键补全



##### 数据库命令

- 查看当前的数据库：db(没有切换数据库的情况下默认使用test数据库)
- 查看所有的数据库：show dbs /show databases
- 切换数据库：use db_name
  - db_name为show dbs后返回的数据库名
- **删除当前的数据库：db.dropDatabase()**



##### 集合命令

- 自动创建集合： 向不存在的集合中第一次添加数据时，集合会自动被创建出来
- 手动创建集合：
  - db.createCollection(name,options)
  - db.createCollection("stu")
  - db.createCollection("sub", { capped : true, size : 10 } )
  - 参数capped：默认值为false表示不设置上限，值为true表示设置上限
  - 参数size：集合所占用的字节数。 当capped值为true时，需要指定此参数，表示上限大小，当文档达到上限时， 会将之前的数据覆盖，单位为字节
- 查看集合：show collections
- 删除集合：db.集合名称.drop()
- 检查集合是否设定上限: db.集合名.isCapped()

    



#### 4.常见的数据类型

- **Object ID： 文档ID/数据的ID，数据的主键**
- String： 字符串，最常用，必须是有效的UTF-8
- Boolean： 存储一个布尔值，true或false
- Integer： 整数可以是32位或64位，这取决于服务器
- Double： 浮点数
- **Arrays： 数组/列表**
- **Object： mongodb中的一条数据/文档，即文档嵌套文档**
- Null： 存储null值
- Timestamp： 时间戳，表示从1970-1-1到现在的总秒数
- Date： 存储当前日期或时间的UNIX时间格式



注意点

- 每个文档都有一个属性，为_id，保证每个文档的唯一性，mongodb默认使用_id作为主键
  - 可以手动设置_id的值，如果没有提供，那么MongoDB为每个文档提供了一个独特的_id， 类型为objectID
- objectID是一个12字节的十六进制数,每个字节两位，一共是24位的字符串：
  - 前4个字节为当前时间戳
  - 接下来3个字节的机器ID
  - 接下来的2个字节中MongoDB的服务进程id
  - 最后3个字节是简单的增量值



#### 5.增加

插入

```
db.stu.insert({name:'gj', gender:1})
db.stu.insert([{name:'gj', gender:1},{name:"laowang",age:40}])
```



保存

```
db.stu.save({_id:"xxxxx",  name:'gj', gender:1})   找的到是修改，找不到是新增
```





#### 6.查询

准备数据

```
db.stu.insert([{"name" : "郭靖", "hometown" : "蒙古", "age" : 20, "gender" : true },
{"name" : "黄蓉", "hometown" : "桃花岛", "age" : 18, "gender" : false },
{"name" : "华筝", "hometown" : "蒙古", "age" : 18, "gender" : false },
{"name" : "黄药师", "hometown" : "桃花岛", "age" : 40, "gender" : true },
{"name" : "段誉", "hometown" : "大理", "age" : 16, "gender" : true },
{"name" : "段王爷", "hometown" : "大理", "age" : 45, "gender" : true },
{"name" : "洪七公", "hometown" : "华筝", "age" : 18, "gender" : true }])
```



##### 6.1 简单查询

- 方法find()： 查询

  `db.集合名称.find({条件文档})`

- 方法findOne()：查询，只返回第一个

  `db.集合名称.findOne({条件文档})`

- 方法pretty()： 将结果格式化；不能和findOne()一起使用！

  `db.集合名称.find({条件文档}).pretty()`



##### 6.2 比较运算符

- 等于： 默认是等于判断， 没有运算符
- 小于：`$lt （less than）`
- 小于等于：`$lte （less than equal）`
- 大于：`$gt （greater than）`
- 大于等于：`$gte`
- 不等于：`$ne`

```
查询年龄大于18的所有学生
db.stu.find({age:{$gte:18}})
```



##### 6.3 逻辑运算符

> 逻辑运算符主要指与、或逻辑

- and：在json中写多个条件即可

```
查询年龄大于或等于18， 并且性别为true的学生
db.stu.find({age:{$gte:18},gender:true})
```

- or:使用$or， 值为数组， 数组中每个元素为json

```
查询年龄大于18， 或性别为false的学生
db.stu.find({$or:[{age:{$gt:18}},{gender:false}]})

查询年龄大于18或性别为男生， 并且姓名是郭靖
db.stu.find({$or:[{age:{$gte:18}},{gender:true}],name:'gj'})
```



##### 6.4 范围运算符

使用`$in`， `$nin` 判断数据是否在某个数组内

```
查询年龄为18、 28的学生
db.stu.find({age:{$in:[18,28,38]}})
```



##### 6.5 支持正则表达式

使用$regex编写正则表达式

```
查询name以'黄'开头的数据
db.stu.find({name:{$regex:'^黄'}})
```



##### 6.6 自定义查询

> mongo shell 是一个js的执行环境 使用$where 写一个函数， 返回满足条件的数据
>
> 条件用this.xxx  

```
查询年龄大于30的学生
db.stu.find({
 $where:function() {
     return this.age>30;}
})
```



##### 6.7 skip和limit

- 方法limit()： 用于读取指定数量的文档

  ```
  db.集合名称.find().limit(NUMBER)
  查询2条学生信息
  db.stu.find().limit(2)
  ```

- 方法skip()： 用于跳过指定数量的⽂档

  ```
  db.集合名称.find().skip(NUMBER)
  db.stu.find().skip(2)
  ```

- 同时使用

  ```
  db.stu.find().limit(4).skip(5)
  db.stu.find().skip(5).limit(4)
  ```

注意：先使用skip在使用limit的效率要高于前者



##### 6.8 投影(难点)

在查询到的返回结果中， 只选择必要的字段

命令：`db.集合名称.find({},{字段名称:1,...})`

**参数为字段与值， 值为1表示显示， 值为0不显 特别注意：**

- 对于_id列默认是显示的， 如果不显示需要明确设置为0
- **对于其他不显示的字段不能设置为0**

```
db.stu.find({},{_id:0,name:1,gender:1})
```



##### 6.9 排序

方法sort()， 用于对查询结果按照指定的字段进行排序

命令：`db.集合名称.find().sort({字段:1,...})`

**参数1为升序排列 ，参数-1为降序排列**

```
根据性别降序， 再根据年龄升序
db.stu.find().sort({gender:-1,age:1})
```



##### 6.10 统计个数

方法count()用于统计结果集中文档条数

命令：`db.集合名称.find({条件}).count()` 

命令：`db.集合名称.count({条件})`

```
db.stu.find({gender:true}).count()
db.stu.count({age:{$gt:20},gender:true})
```



##### 6.11 去重

```
db.stu.distinct("hometown")
```



#### 7.修改(难点)

```
db.集合名称.update({query}, {update}, {multi: boolean})
```

- 参数query: 查询条件
- 参数update: 更新操作符
- 参数multi:可选，默认是false，表示只更新找到的第一条数据，值为true表示把满足条件的数据全部更新

```
db.stu.update({name:'hr'},{name:'mnc'})           # 全文档进行覆盖更新
db.stu.update({name:'hr'},{$set:{name:'hys'}})    # 指定键值更新操作
db.stu.update({},{$set:{gender:0}},{multi:true})  # 更新全部
db.stu.update({name:"liuliu",{$set:{age:100}},{upsert:true}})   # 能找到为更新，否则为新增
```

注意："multi update only works with $ operators"

- multi参数必须和$set一起使用！



#### 8.删除

```
db.集合名称.remove({query}, {justOne: boolean})
	- 参数query:可选，删除的⽂档的条件
	- 参数justOne:可选， 如果设为true或1，则只删除一条，默认false，表示删除全部

e.g.:
	db.stu.remove({"python":"hello mongo"},{justOne:1})
	
```





#### 9.聚合(难点)

##### 常用管道命令

在mongodb中，⽂档处理完毕后， 通过管道进⾏下⼀次处理 常用管道命令如下：

- `$group`： 将集合中的⽂档分组， 可⽤于统计结果
- `$match`： 过滤数据， 只输出符合条件的⽂档
- `$project`： 修改输⼊⽂档的结构， 如重命名、 增加、 删除字段、 创建计算结果
- `$sort`： 将输⼊⽂档排序后输出
- `$limit`： 限制聚合管道返回的⽂档数
- `$skip`： 跳过指定数量的⽂档， 并返回余下的⽂档

##### 常用表达式

表达式：处理输⼊⽂档并输出 语法：`表达式:'$列名'` 常⽤表达式:

- `$sum`： 计算总和， $sum:1 表示以⼀倍计数
- `$avg`： 计算平均值
- `$min`： 获取最⼩值
- `$max`： 获取最⼤值
- `$push`： 在结果⽂档中插⼊值到⼀个数组中



##### $group

```

db.stu.aggregate({$group:{_id:"$gender",counter:{$sum:1},age_sum:{$sum:"$age"},
name_list:{$push:"$name"}}})

```



数据透视

正常情况在统计的不同性别的数据的时候，需要知道所有的name，需要逐条观察，如果通过某种方式把所有的name放到一起，那么此时就可以理解为数据透视



统计不同性别的学生:

```json
 db.stu.aggregate({$group:{_id:null,name:{$push:"$name"}}})
```



##### $match

`$match`用于进行数据的过滤，是在能够在聚合操作中使用的命令，和`find`区别在于`$match` 操作可以把结果交给下一个管道处理，而`find`不行

使用示例如下：

1. 查询年龄大于20的学生

   ```json
    db.stu.aggregate({$match:{age:{$gt:20}}})
   ```

2. 查询年龄大于20的男女学生的人数

   ```json
    db.stu.aggregate(
        {$match:{age:{$gt:20}}},
        {$group:{_id:"$gender",counter:{$sum:1}}}
   )
   ```



##### $project

`$project`用于修改文档的输入输出结构，例如重命名，增加，删除字段

使用示例如下：

查询学生的年龄、姓名 (仅输出年龄姓名)

```json
 db.stu.aggregate(
     {$project:{_id:0,name:1,age:1}}
     )
```



##### $sort

`$sort`用于将输入的文档排序后输出,  1为升序，-1为降序

使用示例如下：

查询学生信息，按照年龄升序

```json
 db.stu.aggregate({$sort:{age:1}})
```



##### limit和skip

- `$limit`限制返回数据的条数
- `$skip` 跳过指定的文档数，并返回剩下的文档数
- 同时使用时先使用skip在使用limit

使用示例如下：

1. 查询2条学生信息

   ```json
    db.stu.aggregate(
        {$limit:2}
    )
   ```

2. 查询从第三条开始的学生信息

   ```json
    db.stu.aggregate(
        {$skip:3}
    )
   ```

3. 统计男女生人数，按照人数升序，返回第二条数据

   ```json
    db.stu.aggregate(
        {$group:{_id:"$gender",counter:{$sum:1}}},
        {$sort:{counter:1}},
        {$skip:1},
        {$limit:1}
    )
   ```



拆分字段

```
db.stu.aggregate({$unwind:{path:"$size",preserveNullAndEmptyArrays:true}})
```



#### 10.索引

##### 创建

```
db.集合名.ensureIndex({属性:1})      1表示升序， -1表示降序
```



##### 对比

插入数据：

```
for　(i=0;i<100000;i++){db.t1.insert({name:'test'+i,age:i})}
```

创建索引前：

```
db.t1.find({name:'test10000'})
db.t1.find({name:'test10000'}).explain('executionStats')
# 显示查询操作的详细信息  55ms
```

创建索引：

```
db.t1.ensureIndex({name:1})
```

创建索引后：

```
db.t1.find({name:'test10000'}).explain('executionStats')  
# 8ms
```



##### 查看

```
db.集合名.getIndexes()
```



##### 删除

```
db.t1.dropIndex({name:1})
db.t1.getIndexes()
```



##### 唯一索引

```
# 根据唯一索引指定的字段的值，如果相同，则无法插入数据

db.t1.ensureIndex({"name":1}, {"unique":true})   
db.t1.insert({name: 'test10000'})   # 如果存在报错
```



##### 联合索引

```
db.collection_name.ensureIndex({字段1:1,字段2:1})
```





#### 11.权限

准备工作

> mongod 启动报错：
>
> mongod
>
> 根据提示进到mongod.lock所在的文件夹，删除mongod.lock
>
> sudo mongod -f mongo.conf 

```
# 在home目录下创建mongo.conf配置文件
vim mongo.conf

# 新增以下内容
dbpath=/home/python/python
logpath=/home/python/python.log
logappend=true
fork=true
auth=true

# 一定记得手动创建上面的dbpath 最后的文件夹 python

# 停掉后台运行的mongod
sudo service mongod stop
ps aux | grep mongod

# 以上面的配置文件运行mongod ,有可能启动报错
sudo mongod -f mongo.conf
```



创建超级管理员用户

```
use admin
db.createUser({"user":"python","pwd":"python","roles":["root"]})

exit
```



创建普通用户

```

mongo

方式一：
# 使用的数据库上创建普通用户
use test1

# 创建普通用户python3,该用户在test1上的权限是只读
db.createUser({"user":"python3", "pwd":"python3", roles:["read"]})

# 创建普通用户python7,该用户在test1上的权限是读写
db.createUser({"user":"python7", "pwd":"python7", roles:["readWrite"]})


方式二：
# admin用户数据库上创建用户并制定权限 (常用)
use admin
db.auth("python","python")

db.createUser({"user":"python37", "pwd":"python37", roles:[{"role":"read","db":"test1"},{"role":"readWrite","db":"test2"}
]})

```

登录

```
# 如果是要通过普通数据库登录，
use test1
db.auth("python3","python3")  / db.auth("python7","python7")
db.test1.find()
db.test1.insert()

# 如果是要通过admin创建的用户登录，
use admin
db.auth("python37","python37")      
use test1
db.test1.find()
db.test1.insert()
```

删除

```
use admin
db.auth("python","python")
db.dropUser('python3')
```



查看用户权限

```
show users
```



#### 12.与python交互

```
pip install pymongo
```



```python
#coding:utf-8
from pymongo import MongoClient

# 创建数据库链接对象
client = MongoClient('172.16.123.223', 27017)

# 选择一个数据库,不需认证的此步骤省略
db = client['admin']

db.authenticate('python','python')

# 选择一个集合
col = client['pydata']['test']

# col.insert({"class":"python37"})
# col.insert([{"class":"python38"},{"class":"python39"},{"class":"python40"}])
for data in col.find():
    print(data)
# print(col.find_one())
print("*"*50)

# 全文档覆盖更新
# col.update({"class":"python40"},{"message":"helloworld"})
# col.update({},{"$set":{"id":"xxxx-xxxx"}})
# col.update({}, {"$set": {"id": "xxxx-xxxx"}}, multi=True)
# col.update({"message":"hello world"}, {"$set": {"id": "xxxx-xxx2"}}, upsert=True)

# col.delete_one({"message":"helloworld"})
col.delete_many({"id": "xxxx-xxxx"})

for data in col.find():
    print(data)
```





## day07

### scrapy

#### 1.scrapy流程

![](images/1.3.3.scrapy工作流程.png)





**其流程可以描述如下：**

1. 爬虫中起始的url构造成request对象-->爬虫中间件-->引擎-->调度器
2. 调度器把request-->引擎-->下载中间件--->下载器
3. 下载器发送请求，获取response响应---->下载中间件---->引擎--->爬虫中间件--->爬虫
4. 爬虫提取url地址，构建request对象， 重复步骤1
5. 爬虫提取数据--->引擎--->管道处理和保存数据



#### 2.简单使用

**安装**

```
pip install scrapy 
```

**开发流程**

```
创建项目:
    scrapy startproject mySpider
    
生成一个爬虫:
   cd mySpider
    scrapy genspider itcast itcast.cn
   	
    
提取数据:
    根据网站结构在spider中实现数据采集相关内容
    	# name             爬虫名
    	# allowed_domains  域名
    	# start_url        起始地址
    	# parse 方法        请求解析方法
    
保存数据:
    使用pipeline进行数据后续处理和保存
    
    
在settings.py配置启用管道
	ITEM_PIPELINES = {
	    'myspider.pipelines.ItcastPipeline': 400
	}
	
运行scrapy
	scrapy crawl itcast --nolog
```



**创建项目后的文件说明**：

![](images/2.1.scrapy入门使用-1.png)



**生成爬虫的文件目录说明**：

![](images/2.2.scrapy入门使用-2.png)





案例1： 传智官网

在/myspider/myspider/spiders/itcast.py中修改内容如下:



> 1. response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法
> 2. 额外方法extract()：返回一个包含有字符串的列表
> 3. 额外方法extract_first()：返回列表中的第一个字符串，列表为空没有返回None

```
import scrapy

class ItcastSpider(scrapy.Spider):  # 继承scrapy.spider
    # 爬虫名字 
    name = 'itcast' 
    # 允许爬取的范围
    allowed_domains = ['itcast.cn'] 
    # 开始爬取的url地址
    start_urls = ['http://www.itcast.cn/channel/teacher.shtml']

    # 数据提取的方法，接受下载中间件传过来的response
    def parse(self, response): 
        # scrapy的response对象可以直接进行xpath
        names = response.xpath('//div[@class="tea_con"]//li/div/h3/text()') 
        print(names)

        # 获取具体数据文本的方式如下
        # 分组
        li_list = response.xpath('//div[@class="tea_con"]//li') 
        for li in li_list:
            # 创建一个数据字典
            item = {}
            # 利用scrapy封装好的xpath选择器定位元素，并通过extract()或extract_first()来获取结果
            item['name'] = li.xpath('.//h3/text()').extract_first() # 老师的名字
            item['level'] = li.xpath('.//h4/text()').extract_first() # 老师的级别
            item['text'] = li.xpath('.//p/text()').extract_first() # 老师的介绍
            print(item)
```



在pipelines.py文件中定义对数据的操作

```
import json

class ItcastPipeline():
    # 爬虫文件中提取数据的方法每yield一次item，就会运行一次
    # 该方法为固定名称函数
    def process_item(self, item, spider):
        print(item)
        return item
```



在settings.py配置启用管道

```
ITEM_PIPELINES = {
    'myspider.pipelines.ItcastPipeline': 400
}
```



运行：

```
scrapy crawl itcast   /   scrapy crawl itcast --nolog
```



#### 3.数据建模

> 1. 定义item即提前规划好哪些字段需要抓，防止手误，因为定义好之后，在运行过程中，系统会自动检查
> 2. 配合注释一起可以清晰的知道要抓取哪些字段，没有定义的字段不能抓取，在目标字段少的时候可以使用字典代替



建模

在items.py文件中定义要提取的字段：

```

class MyspiderItem(scrapy.Item): 
    name = scrapy.Field()   # 讲师的名字
    title = scrapy.Field()  # 讲师的职称
    desc = scrapy.Field()   # 讲师的介绍
```

使用

```
from myspider.items import MyspiderItem   # 导入Item，注意路径

    def parse(self, response)

        item = MyspiderItem() # 实例化后可直接使用

        item['name'] = node.xpath('./h3/text()').extract_first()
        item['title'] = node.xpath('./h4/text()').extract_first()
        item['desc'] = node.xpath('./p/text()').extract_first()

        print(item)
```



注意：

​	pipeline.py 将json.dumps(items,ensure_ascii=false) 中的item 转成字典再传入



#### 4.翻页

实现方法

1. 确定url地址
2. 构造请求，scrapy.Request(url,callback)
   - callback：指定解析函数名称，表示该请求返回的响应使用哪一个函数进行解析
3. 把请求交给引擎：yield scrapy.Request(url,callback)



案例： 网易招聘

1. 可以在settings中设置ROBOTS协议

```
# False表示忽略网站的robots.txt协议，默认为True
ROBOTSTXT_OBEY = False
```

2. 可以在settings中设置User-Agent: 

```
# scrapy发送的每一个请求的默认UA都是设置的这个User-Agent
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36' 
```

3. 在爬虫文件的parse方法中: 

   wangyi/spiders/job.py

```
import scrapy

class JobSpider(scrapy.Spider):
    name = 'job'
    # 2.检查允许的域名
    allowed_domains = ['163.com']
    # 1 设置起始的url
    start_urls = ['https://hr.163.com/position/list.do']

    def parse(self, response):
        # 获取所有的职位节点列表
        node_list = response.xpath('//*[@class="position-tb"]/tbody/tr')
        # print(len(node_list))

        # 遍历所有的职位节点列表
        for num, node in enumerate(node_list):
            # 索引为值除2取余为0的才是含有数据的节点，通过判断进行筛选
            if num % 2 == 0:
                item = {}

                item['name'] = node.xpath('./td[1]/a/text()').extract_first()
                item['link'] = node.xpath('./td[1]/a/@href').extract_first()
                item['depart'] = node.xpath('./td[2]/text()').extract_first()
                item['category'] = node.xpath('./td[3]/text()').extract_first()
                item['type'] = node.xpath('./td[4]/text()').extract_first()
                item['address'] = node.xpath('./td[5]/text()').extract_first()
                item['num'] = node.xpath('./td[6]/text()').extract_first().strip()
                item['date'] = node.xpath('./td[7]/text()').extract_first()
                yield item

        # 翻页处理
        # 获取翻页url
        part_url = response.xpath('//a[contains(text(),">")]/@href').extract_first()

        # 判断是否为最后一页，如果不是最后一页则进行翻页操作
        if part_url != 'javascript:void(0)':
            # 拼接完整翻页url
            next_url = 'https://hr.163.com/position/list.do' + part_url

            yield scrapy.Request(
                url=next_url,
                callback=self.parse
            )
```



wangyi/items.py

```
class WangyiItem(scrapy.Item):
    # define the fields for your item here like:

    name = scrapy.Field()
    link = scrapy.Field()
    depart = scrapy.Field()
    category = scrapy.Field()
    type = scrapy.Field()
    address = scrapy.Field()
    num = scrapy.Field()
    date = scrapy.Field()
```



#### 5.scrapy.Request参数

```python
scrapy.Request(url[,callback,method="GET",headers,body,cookies,meta,dont_filter=False])
```

参数解释

1. 括号里的参数为可选参数
2. **callback**：表示当前的url的响应交给哪个函数去处理
3. **meta**：实现数据在不同的解析函数中传递，meta默认带有部分数据，比如下载延迟，请求深度等
4. dont_filter:默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动
5. method：指定POST或GET请求
6. headers：接收一个字典，其中不包括cookies
7. cookies：接收一个字典，专门放置cookies
8. body：接收json字符串，为POST的数据，发送payload_post请求时使用（在下一章节中会介绍post请求）



meta参数使用

>meta的作用：meta可以实现数据在不同的解析函数中的传递
>
>meta参数是一个字典



在爬虫文件的parse方法中，提取详情页增加之前callback指定的parse_detail函数：

```

def parse(self,response):
    yield scrapy.Request(detail_url, callback=self.parse_detail, meta={"item":item})


def parse_detail(self,response):
    #获取之前传入的item
    item = resposne.meta["item"]
    item["duty"] = response.xpath().ex
    yield item
```



#### 6.scrapy携带cookie

> 通过重写 start_requests 来进行cookie的传递



案例: github

```python
import scrapy

class Git1Spider(scrapy.Spider):
    name = 'git1'
    allowed_domains = ['github.com']
    start_urls = ['https://github.com/exile-morganna']

    def start_requests(self):
        url = self.start_urls[0]
        temp = '_ga=GA1.2.1190047373.1543731773; _octo=GH1.1.1199554731.1543731773; user_session=6RCB6AkOT97lY9QXs98mHgHY6m8IScKjQPsf0i70K6GmSeeM; __Host-user_session_same_site=6RCB6AkOT97lY9QXs98mHgHY6m8IScKjQPsf0i70K6GmSeeM; logged_in=yes; dotcom_user=exile-morganna; _device_id=337049c7fc9520c4dbf2b5bdae3f2324; tz=Asia%2FShanghai; has_recent_activity=1; _gh_sess=bFl1RVN1RE9WTlBRVWthWjlBTFBaK0cySWRrV3lVL1FNTWdONG02WnFDTXpqWTVmeXdzWnI0Tk83dUtJY3BMc0xhMWdseHU4aDMwemFoMzRzNnB6UHZpV1hzUk8zbzlhNXozMURCdFozSTZtaWFOWEZmWklsb0FoK2NlM0IrWHRTcGtNUEJnbDlLOWFxSHpCVW1JY1ZxaFBDWlZCcEp5L00wOG5ycnhWWGcrSHZHOEZCdjdIK1ZxM1FBODkyT1JBbGxUTTJ6RTlLZDN0KytVRndXaWc1T0ErUTJzKzhpRGFpVVA0M3ZQSTN2d2prN2VzYjNSR1o1SXdUVURPTjRYYi0tcFJ1S0FoQVRSSE9INVZuOHlwUmRVZz09--d4ff9d6202137936d6c3968e2e5ef5a5b1f54ce7'
        cookies = {data.split('=')[0]:data.split('=')[-1]for data in temp.split('; ')}

        yield scrapy.Request(
            url=url,
            cookies=cookies
        )

    def parse(self, response):
        with open("git_with_cookies.html", "w")as f:
            f.write(response.text)

```

注意：

在setting中设置ROBOTS协议、USER_AGENT



#### 7.scrapy发送post请求

案例： github

```python
# -*- coding": "utf-8 -*-
import scrapy


class Git2Spider(scrapy.Spider):
    name = 'git2'
    allowed_domains = ['github.com']
    start_urls = ['https://github.com/login']

    def parse(self, response):
        token = response.xpath('//input[@name="authenticity_token"]/@value').extract_first()

        # 构造post数据
        post_data = {
            "commit": "Sign in",
            "utf8": "✓",
            "authenticity_token": token,
            "login": "exile-morganna",
            "password": "1QAZ2wSX3edC4rfv",
            "webauthn-support": "supported"
        }
        print(post_data)

        # 构造一个post请求对象
        yield scrapy.FormRequest(
            url='https://github.com/session',
            callback=self.login,
            formdata=post_data
        )

    def login(self, response):
        yield scrapy.Request(
            url='https://github.com/exile-morganna',
            callback=self.check_login
        )

    def check_login(self, response):
        with open("git_with_post.html", "w")as f:
            f.write(response.text)

```





#### 8.scrapy管道的使用

常用的方法：

1. process_item(self,item,spider):
   - 管道类中必须有的函数
   - 实现对item数据的处理
   - 必须return item
2. open_spider(self, spider): 在爬虫开启的时候仅执行一次
3. close_spider(self, spider): 在爬虫关闭的时候仅执行一次



wangyi/wangyi/pipelines.py

```python
class WangyiSimplePipeline(object):

    def open_spider(self, spider):
        if spider.name == 'job_simple':
            self.file = open('wangyi_simple.json', 'w')

    def process_item(self, item, spider):
        if spider.name == 'job_simple':
            item = dict(item)

            str_data = json.dumps(item, ensure_ascii=False) + ',\n'

            self.file.write(str_data)

        return item

    def close_spider(self, spider):
        if spider.name == 'job_simple':
            self.file.close()

```



在settings.py设置开启pipeline

```

ITEM_PIPELINES = {
    'myspider.pipelines.ItcastFilePipeline': 400, # 400表示权重
    'myspider.pipelines.ItcastMongoPipeline': 500, # 权重值越小，越优先执行！
}

```



#### 9.crawlspider

> **crawlspider爬虫可以按照规则自动获取连接**



创建crawlspider

```
scrapy genspider -t crawl job_crawl 163.com
```



案例:  网易招聘

```python
# -*- coding: utf-8 -*-
import scrapy
# 链接提取器
from scrapy.linkextractors import LinkExtractor
# 爬虫类 规则
from scrapy.spiders import CrawlSpider, Rule


class JobCrawlSpider(CrawlSpider):
    name = 'job_crawl'
    allowed_domains = ['163.com']
    start_urls = ['https://hr.163.com/position/list.do']

    # 链接处理规则
    rules = (
        # follow 决定是否继续在链接提取器提取的链接对应的响应中继续应用链接提取器，一般持续翻页的链接提取规则需要设置为true
        Rule(LinkExtractor(allow=r'\?currentPage=\d+$'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):

        node_list = response.xpath('//*[@class="position-tb"]/tbody/tr')
        # print(len(node_list))

        # 遍历节点列表
        for num, node in enumerate(node_list):
            # 设置过滤条件，将目标节点获取出来
            if num % 2 == 0:
                item = {}

                item['name'] = node.xpath('./td[1]/a/text()').extract_first()
                # response.urljoin()用于拼接相对路径的url，可以理解成自动补全
                item['link'] = response.urljoin(node.xpath('./td[1]/a/@href').extract_first())
                item['depart'] = node.xpath('./td[2]/text()').extract_first()
                item['category'] = node.xpath('./td[3]/text()').extract_first()
                item['type'] = node.xpath('./td[4]/text()').extract_first()
                item['address'] = node.xpath('./td[5]/text()').extract_first()
                item['num'] = node.xpath('./td[6]/text()').extract_first().strip()
                item['date'] = node.xpath('./td[7]/text()').extract_first()
                yield item


```



运行：

```
scrapy crawl job_crawl
```



跟普通的scrapy.spider的区别：

```
在crawlspider爬虫中，没有parse函数
```



LinkExtractor 更多参数:

- allow: 满足括号中的're'表达式的url会被提取，如果为空，则全部匹配
- deny: 满足括号中的're'表达式的url不会被提取，优先级高于allow
- allow_domains: 会被提取的链接的domains(url范围)，如：`['hr.tencent.com', 'baidu.com']`
- deny_domains: 不会被提取的链接的domains(url范围)
- **restrict_xpaths: 使用xpath规则进行匹配，即xpath满足的范围内的url地址会被提取**，如：`restrict_xpaths='//div[@class="pagenav"]'`





#### 10.中间件

**分类**

> 根据位置不同，做的分类

1. 下载中间件
2. 爬虫中间件



**作用**： 预处理request和response对象

1. 对header以及cookie进行更换和处理
2. 使用代理ip等
3. 对请求进行定制化操作

但在scrapy默认的情况下 两种中间件都在middlewares.py一个文件中

爬虫中间件使用方法和下载中间件相同，且功能重复，通常使用下载中间件



**中间件方法**

Downloader Middlewares默认的方法：

- process_request(self, request, spider)：
  1. 当每个request通过下载中间件时，该方法被调用。
  2. 返回None值：没有return也是返回None，该request对象传递给下载器，或通过引擎传递给其他权重低的process_request方法
  3. 返回Response对象：不再请求，把response返回给引擎
  4. 返回Request对象：把request对象通过引擎交给调度器，此时将不通过其他权重低的process_request方法
- process_response(self, request, response, spider)：
  1. 当下载器完成http请求，传递响应给引擎的时候调用
  2. 返回Resposne：通过引擎交给爬虫处理或交给权重更低的其他下载中间件的process_response方法
  3. 返回Request对象：通过引擎交给调取器继续请求，此时将不通过其他权重低的process_request方法
- 在settings.py中配置开启中间件，权重值越小越优先执行



**随机请求头**

Douban/Douban/settings.py

```
USER_AGENT_LIST = [
"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.5 (KHTML, like Gecko) Chrome/4.0.249.0 Safari/532.5 ",
"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14 ",
"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20 ",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.27 (KHTML, like Gecko) Chrome/12.0.712.0 Safari/534.27 ",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1 ",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7 ",
"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1 ",
"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1 ",
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0a2) Gecko/20110622 Firefox/6.0a2 ",
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:7.0.1) Gecko/20100101 Firefox/7.0.1 ",
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0b4pre) Gecko/20100815 Minefield/4.0b4pre ",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.8 (KHTML, like Gecko) Beamrise/17.2.0.9 Chrome/17.0.939.0 Safari/535.8 ",
"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/18.6.872.0 Safari/535.2 UNTRUSTED/1.0 3gpp-gba"
]

```



Douban/Douban/middlewares.py

```
class RandomUserAgent(object):

    def process_request(self, request, spider):

        # print(request.headers['User-Agent'])
        ua = random.choice(USER_AGENT_LIST)
        request.headers['User-Agent'] = ua
```



**随机ip代理**

> request.meta中增加`proxy`字段



Douban/middlewares.py

```
class RandomProxy(object):

    def process_request(self, request, spider):
        proxy = random.choice(PROXY_LIST)
        print(proxy)

        # 判断是免费还是收费的代理
        if 'user_passwd' in proxy:
            # 对账号密码进行编码，python3中base64编码的数据必须是bytes类型，所以需要encode
            b64_up = base64.b64encode(proxy['user_passwd'].encode())
            # 设置认证
            request.headers['Proxy-Authorization'] = 'Basic ' + b64_up.decode()
            # 设置代理
            request.meta['proxy'] = proxy['ip_port']
        else:
            # 设置代理
            request.meta['proxy'] = proxy['ip_port']
```



Douban/settings.py

```
PROXY_LIST =[
   {"ip_port": "123.207.53.84:16816", "user_passwd": "morganna_mode_g:ggc22qxp"},
   {"ip_port": "122.234.206.43:9000"},
]

```



**动态加载selenium**

案例：aqi（pm2.5）

> 为什么要用selenium ：　从其他地方提取不到数据，要从渲染后的页面中提取数据
>



AQI/AQI/middlewares.py

```
class SeleniumMiddleware(object):

    def process_request(self, request, spider):
        url = request.url

        if 'daydata' in url:
            driver = webdriver.Chrome()

            driver.get(url)
            time.sleep(3)
            data = driver.page_source

            driver.close()

            # 创建响应对象　
            res = HtmlResponse(url=url, body=data, encoding='utf-8', request=request)

            return res
```

AQI/AQI/settings.py

```
DOWNLOADER_MIDDLEWARES = {
   # 'AQI.middlewares.MyCustomDownloaderMiddleware': 543,
   'AQI.middlewares.SeleniumMiddleware': 543,
}

```







## day08

### scrapy_redis

> 数据队列，集合，任务队列， 通过redis-key(启动爬虫)

#### 1.流程

- 在scrapy_redis中，**所有的待抓取的request对象和去重的request对象指纹都存在所有的服务器公用的redis中**
- 所有的服务器中的scrapy进程公用同一个redis中的request对象的队列
- 所有的request对象存入redis前，都会通过该redis中的**request指纹集合进行判断，之前是否已经存入过**
- 在默认情况下所有的**数据**会保存在redis中(数据队列)



![](images/7.4.2.scrapy_redis的流程.png)





#### 2.断点续爬

> 下载源码  git clone https://github.com/rolando/scrapy-redis.git
>
> 虚拟环境中　pip install scrapy-redis　
>
> 安装redis-desktop(见课件资料)





scrapy-redis/example-project/example/spiders/dmoz.py

```
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class DmozSpider(CrawlSpider):
    """Follow categories and extract links."""
    name = 'dmoz'
    allowed_domains = ['dmoztools.net']
    start_urls = ['http://www.dmoztools.net/']

    rules = [
        Rule(LinkExtractor(
            restrict_css=('.top-cat', '.sub-cat', '.cat-item')
        ), callback='parse_directory', follow=True),
    ]

    def parse_directory(self, response):
        for div in response.css('.title-and-desc'):
            yield {
                'name': div.css('.site-title::text').extract_first(),
                'description': div.css('.site-descr::text').extract_first().strip(),
                'link': div.css('a::attr(href)').extract_first(),
            }

```

注意：　如果要运行，需要更改

 allowed_domains = ['dmoztools.net']
 start_urls = ['http://www.dmoztools.net/']



scrapy-redis/example-project/example/settings.py

```
# 设置重复过滤器的模块
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
# 设置调取器，scrap_redis中的调度器具备与数据库交互的功能
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
# 设置当爬虫结束的时候是否保持redis数据库中的去重集合与任务队列
SCHEDULER_PERSIST = True


ITEM_PIPELINES = {
    'example.pipelines.ExamplePipeline': 300,
    # 当开启该管道，该管道将会把数据存到Redis数据库中
    'scrapy_redis.pipelines.RedisPipeline': 400,
}
# 设置redis数据库
REDIS_URL = "redis://127.0.0.1:6379"

LOG_LEVEL = 'DEBUG'

# Introduce an artifical delay to make use of parallelism. to speed up the
# crawl.
DOWNLOAD_DELAY = 0.5
```

启动，停掉，再启动爬虫，　观察redis-desktop中　redis１数据变化

> 运行方式　参考readme.rst  
>
> scrapy crawl dmoz





#### 3.源码分析

- RedisPipeline       管道类
- RFPDupeFilter     指纹去重类
- Scheduler            调度器类
  - request的指纹不在集合中
  - request的dont_filter为True，即不过滤



#### 4.分布式爬虫

> scrapy-redis项目中的mycrawler_redis.py文件　需要将__init__ 中的filter 强转为list , list(filter(none,domain))
>
> 
>
> 运行: 
>
> cd scrapy-redis/example-project/example/spiders
>
> scrapy runspider myspider.py
>
> redis-cli
>
> ​	 lpush py21 https://www.baidu.com



编写分布式爬虫的步骤：

1. 改造爬虫

   1. 导入scrapy-redis中的爬虫类
   2. 继承类
   3. 注释掉　start_url ＆ allowed_domains
   4. 设置redis-key 获取 start_url
   5. 设置__init__ 　获取允许的域

   

2. 改造配置文件

   settings.py

   ```
   # 设置重复过滤器的模块
   DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
   
   # 设置调取器，scrap_redis中的调度器具备与数据库交互的功能
   SCHEDULER = "scrapy_redis.scheduler.Scheduler"
   
   # 设置当爬虫结束的时候是否保持redis数据库中的去重集合与任务队列
   SCHEDULER_PERSIST = True
   
   ITEM_PIPELINES = {
       # 当开启该管道，该管道将会把数据存到Redis数据库中
       'scrapy_redis.pipelines.RedisPipeline': 400,
   }
   
   # 设置redis数据库
   REDIS_URL = "redis://127.0.0.1:6379"
   
   # Introduce an artifical delay to make use of parallelism. to speed up the
   # crawl.
   DOWNLOAD_DELAY = 0.5
   
   ```







案例: 京东图书

JD/spiders/book.py

```python
# -*- coding: utf-8 -*-
import scrapy
from JD.items import JdItem
import json

# ----1 导入分布式爬虫类
from scrapy_redis.spiders import RedisSpider


# ----2 继承分布式爬虫类
class BookSpider(RedisSpider):
    name = 'book'

    # ----3 注销start_urls&allowed_domains
    # # 修改允许的域
    # allowed_domains = ['jd.com', 'p.3.cn']
    # # 修改起始的url
    # start_urls = ['https://book.jd.com/booksort.html']

    # ----4 设置redis-key
    redis_key = 'py21'

    # ----5 设置__init__
    def __init__(self, *args, **kwargs):
        domain = kwargs.pop('domain', '')
        self.allowed_domains = list(filter(None, domain.split(',')))
        super(BookSpider, self).__init__(*args, **kwargs)

    def parse(self, response):
        # 获取所有图书大分类节点列表
        big_node_list = response.xpath('//*[@id="booksort"]/div[2]/dl/dt/a')

        for big_node in big_node_list[:1]:

            big_category = big_node.xpath('./text()').extract_first()
            big_category_link = response.urljoin(big_node.xpath('./@href').extract_first())

            # 获取所有图书小分类节点列表
            small_node_list = big_node.xpath('../following-sibling::dd[1]/em/a')

            for small_node in small_node_list[:1]:
                temp = {}

                temp['big_category'] = big_category
                temp['big_category_link'] = big_category_link
                temp['small_category'] = small_node.xpath('./text()').extract_first()
                temp['small_category_link'] = response.urljoin(small_node.xpath('./@href').extract_first())

                # 模拟点击小分类链接
                yield scrapy.Request(
                    url=temp['small_category_link'],
                    callback=self.parse_book_list,
                    meta={"py21": temp}
                )

    def parse_book_list(self, response):
        temp = response.meta['py21']

        book_list = response.xpath('//*[@id="plist"]/ul/li/div')
        # print(len(book_list))

        for book in book_list:
            item = JdItem()

            item['big_category'] = temp['big_category']
            item['big_category_link'] = temp['big_category_link']
            item['small_category'] = temp['small_category']
            item['small_category_link'] = temp['small_category_link']

            # 部分书籍的书名和作者规则不同
            item['bookname'] = book.xpath(
                './div[3]/a/em/text()|./div/div[2]/div[2]/div[3]/a/em/text()').extract_first().strip()
            item['author'] = book.xpath(
                './div[4]/span[1]/span/a/text()|./div/div[2]/div[2]/div[4]/span[1]/span[1]/a/text()').extract_first().strip()
            item['link'] = book.xpath('./div[1]/a/@href|./div/div[2]/div[2]/div[1]/a/@href').extract_first()

            # 获取图书编号
            """
            https://p.3.cn/prices/mgets?skuIds=J_
            url编解码
            """
            skuid = book.xpath('.//@data-sku').extract_first()
            # 下面的套装提取不到
            # skuid = book.xpath('./@data-sku').extract_first()　　
            # print("skuid:",skuid)
            # 拼接图书价格低至
            pri_url = 'https://p.3.cn/prices/mgets?skuIds=J_' + skuid
            yield scrapy.Request(url=pri_url, callback=self.parse_price, meta={'meta_1': item})
            break

    def parse_price(self, response):
        # 记得修改允许的域
        item = response.meta['meta_1']

        # response.body是　byte
        dict_data = json.loads(response.text)
        print(dict_data)

        item['price'] = dict_data[0]['p']
        yield item
```



JD/settings.py

```python
# Scrapy settings for example project
#
# For simplicity, this file contains only the most important settings by
# default. All the other settings are documented here:
#
#     http://doc.scrapy.org/topics/settings.html
#
SPIDER_MODULES = ['JD.spiders']
NEWSPIDER_MODULE = 'JD.spiders'

USER_AGENT = 'scrapy-redis (+https://github.com/rolando/scrapy-redis)'

# 设置重复过滤器的模块
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
# 设置调取器，scrap_redis中的调度器具备与数据库交互的功能
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
# 设置当爬虫结束的时候是否保持redis数据库中的去重集合与任务队列
SCHEDULER_PERSIST = True
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack"

ITEM_PIPELINES = {
    # 'JD.pipelines.ExamplePipeline': 300,
    # 当开启该管道，该管道将会把数据存到Redis数据库中
    'scrapy_redis.pipelines.RedisPipeline': 400,
}
# 设置redis数据库
REDIS_URL = "redis://127.0.0.1:6379"

# LOG_LEVEL = 'DEBUG'

# Introduce an artifical delay to make use of parallelism. to speed up the
# crawl.
DOWNLOAD_DELAY = 1

```



运行

```
cd JD/JD/spiders
scrapy runspider book.py

redis-cli
    lpush py21 https://book.jd.com/booksort.html
```









## day09

### scrapy-splash

#### 1.简介

scrapy_splash是scrapy的一个组件,能够模拟浏览器加载js，并返回js运行后的数据

- scrapy-splash加载js数据是基于Splash来实现的。
- Splash是一个Javascript渲染服务。它是一个实现了HTTP API的轻量级浏览器，Splash是用Python和Lua语言实现的，基于Twisted和QT等模块构建。
- 使用scrapy-splash最终拿到的response相当于是在浏览器全部渲染完成以后的网页源代码。
- splash官方文档 <https://splash.readthedocs.io/en/stable/>
- scrapy_splash中文文档   <https://splash-cn-doc.readthedocs.io/zh_CN/latest/scrapy-splash-toturial.html>



#### 2.安装

安装docker(已安装可忽略)

```
step1:
sudo apt-get update

step2:
sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common

step3:
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

step4:
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"


step5:
sudo apt-get update

step6:
sudo apt-get install docker-ce

step7:
sudo docker run hello-world

```

拉取镜像(镜像已提供给大家，不要去下载)

```
＃　sudo docker pull scrapinghub/splash
# 命令行切换到scrapy_splash_docker.tar.gz　所在目录

docker load ./scrapy_splash_docker.tar.gz
```

运行

```
sudo docker run -p 8050:8050 scrapinghub/splash

 http://127.0.0.1:8050  看到splash 界面，加载较慢
```



#### 3.使用

```
＃　安装环境

pip install scrapy-splash
```

```
# 创建项目

scrapy startproject test_splash
cd test_splash
scrapy genspider baidu baidu.com
scrapy genspider baidu_ws baidu.com
```



Splash/Splash/spiders/baidu_ws.py

> 重写start_requests

```python
# -*- coding: utf-8 -*-
import scrapy
from scrapy_splash.request import SplashRequest


class BaiduWsSpider(scrapy.Spider):
    name = 'baidu_ws'
    allowed_domains = ['baidu.com']
    start_urls = ['https://www.baidu.com/s?wd=python']

    def start_requests(self):
        yield SplashRequest(
            url=self.start_urls[0],
            callback=self.parse,
            args={'wait': 10},  # 最大超时时间，单位：秒
            endpoint='render.html'
        )

    def parse(self, response):
        with open('with_splash.html', 'wb')as f:
            f.write(response.body.decode())
```



Splash/Splash/settings.py

```python
# 渲染服务的url
SPLASH_URL = 'http://127.0.0.1:8050'

# 下载器中间件
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}

# 去重过滤器
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'

# 使用Splash的Http缓存
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False
```



总结：

1. splash类似selenium，能够像浏览器一样访问请求对象中的url地址
2. 能够按照该url对应的响应内容依次发送请求
3. 并将多次请求对应的多次响应内容进行渲染
4. 最终返回渲染后的response响应对象





#### 4.配置

scrapy配置

```
ROBOTSTXT_OBEY 是否遵守robots协议，默认是遵守
关于robots协议
在百度搜索中，不能搜索到淘宝网中某一个具体的商品的详情页面，这就是robots协议在起作用
Robots协议：网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取，但它仅仅是互联网中的一般约定
例如：淘宝的robots协议
USER_AGENT 设置ua
DEFAULT_REQUEST_HEADERS 设置默认请求头，这里加入了USER_AGENT将不起作用
ITEM_PIPELINES 管道，左位置右权重：权重值越小，越优先执行
SPIDER_MIDDLEWARES 爬虫中间件，设置过程和管道相同
DOWNLOADER_MIDDLEWARES 下载中间件
COOKIES_ENABLED 默认为True表示开启cookie传递功能，即每次请求带上前一次的cookie，做状态保持
COOKIES_DEBUG 默认为False表示日志中不显示cookie的传递过程
LOG_LEVEL 默认为DEBUG，控制日志的等级
LOG_LEVEL = "WARNING"
LOG_FILE 设置log日志文件的保存路径，如果设置该参数，日志信息将写入文件，终端将不再显示，且受到LOG_LEVEL日志等级的限制
LOG_FILE = "./test.log"

CONCURRENT_REQUESTS = 32 设置并发数
DOWNLOAD_DELAY 下载延迟，默认无延迟，单位为秒
```



scrapy_redis配置

```
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" # 指纹生成以及去重类
SCHEDULER = "scrapy_redis.scheduler.Scheduler" # 调度器类
SCHEDULER_PERSIST = True # 持久化请求队列和指纹集合
ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline': 400} # 数据存入redis的管道
REDIS_URL = "redis://host:port" # redis的url
```



scrapy_splash配置

```
SPLASH_URL = 'http://127.0.0.1:8050'
DOWNLOADER_MIDDLEWARES = {
'scrapy_splash.SplashCookiesMiddleware': 723,
'scrapy_splash.SplashMiddleware': 725,
'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter' 
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
```



scrapy_redis和scrapy_splash配合使用的配置

> - scrapy-redis中配置了”DUPEFILTER_CLASS” : “scrapy_redis.dupefilter.RFPDupeFilter”，与scrapy-splash配置的DUPEFILTER_CLASS = ‘scrapy_splash.SplashAwareDupeFilter’ 相冲突！
> - 查看了scrapy_splash.SplashAwareDupeFilter源码后，发现他继承了scrapy.dupefilter.RFPDupeFilter，并重写了request_fingerprint()方法。
> - 比较scrapy.dupefilter.RFPDupeFilter和scrapy_redis.dupefilter.RFPDupeFilter中的request_fingerprint()方法后，发现是一样的，因此重写了一个SplashAwareDupeFilter，继承scrapy_redis.dupefilter.RFPDupeFilter，其他代码不变。



**重写去重类**



xxx.py

```
from __future__ import absolute_import

from copy import deepcopy

from scrapy.utils.request import request_fingerprint
from scrapy.utils.url import canonicalize_url

from scrapy_splash.utils import dict_hash

from scrapy_redis.dupefilter import RFPDupeFilter


def splash_request_fingerprint(request, include_headers=None):
    """ Request fingerprint which takes 'splash' meta key into account """

    fp = request_fingerprint(request, include_headers=include_headers)
    if 'splash' not in request.meta:
        return fp

    splash_options = deepcopy(request.meta['splash'])
    args = splash_options.setdefault('args', {})

    if 'url' in args:
        args['url'] = canonicalize_url(args['url'], keep_fragments=True)

    return dict_hash(splash_options, fp)


class SplashAwareDupeFilter(RFPDupeFilter):
    """
    DupeFilter that takes 'splash' meta key in account.
    It should be used with SplashMiddleware.
    """
    def request_fingerprint(self, request):
        return splash_request_fingerprint(request)


"""以上为重写的去重类，下边为爬虫代码"""

from scrapy_redis.spiders import RedisSpider
from scrapy_splash import SplashRequest


class SplashAndRedisSpider(RedisSpider):
    name = 'splash_and_redis'
    allowed_domains = ['baidu.com']

    # start_urls = ['https://www.baidu.com/s?wd=13161933309']
    redis_key = 'splash_and_redis'
    # lpush splash_and_redis 'https://www.baidu.com'

    # 分布式的起始的url不能使用splash服务!
    # 需要重写dupefilter去重类!

    def parse(self, response):
        yield SplashRequest('https://www.baidu.com/s?wd=13161933309',
                            callback=self.parse_splash,
                            args={'wait': 10}, # 最大超时时间，单位：秒
                            endpoint='render.html') # 使用splash服务的固定参数

    def parse_splash(self, response):
        with open('splash_and_redis.html', 'w') as f:
            f.write(response.body.decode())
```

**配置**

```
# 渲染服务的url
SPLASH_URL = 'http://127.0.0.1:8050'
# 下载器中间件
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
# 使用Splash的Http缓存
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'

# 去重过滤器
# DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
# DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" # 指纹生成以及去重类

DUPEFILTER_CLASS = 'xxx.SplashAwareDupeFilter' # 混合去重类的位置

SCHEDULER = "scrapy_redis.scheduler.Scheduler" # 调度器类
SCHEDULER_PERSIST = True # 持久化请求队列和指纹集合, scrapy_redis和scrapy_splash混用使用splash的DupeFilter!
ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline': 400} # 数据存入redis的管道
REDIS_URL = "redis://127.0.0.1:6379" # redis的url
```



#### 5.scrapyd部署

简介

```
scrapyd是一个用于部署和运行scrapy爬虫的程序，它允许你通过JSON API来部署爬虫项目和控制爬虫运行，scrapyd是一个守护进程，监听爬虫的运行和请求，然后启动进程来执行它们
```



安装

```
scrapyd服务: pip install scrapyd

scrapyd客户端: pip install scrapyd-client
```



部署

```

# 1.　更改配置
 [deploy:部署名(部署名可以自行定义)]
 url = http://localhost:6800/
 project = 项目名(创建爬虫项目时使用的名称)
 
 
 # 2.scrapyd 部署
 # 保证scrapyd 没有关闭
 scrapyd-deploy 部署名(配置文件中设置的名称) -p 项目名称
```



启动及关闭爬虫

> 注意替换自己的信息

```
＃　启动
	curl http://localhost:6800/schedule.json -d project=project_name -d spider=spider_name

	例如：
	curl http://localhost:6800/schedule.json -d project=Splash -d spider=baidu

＃　关闭
	curl http://localhost:6800/cancel.json -d project=project_name -d job=jobid

	例如：
	curl http://localhost:6800/cancel.json -d project=Splash -d job=xxx


```



#### 6.Gerapy

介绍

```
Gerapy 是一款 分布式爬虫管理框架，支持 Python 3，基于 Scrapy、Scrapyd、Scrapyd-Client、Scrapy-Redis、Scrapyd-API、Scrapy-Splash、Jinjia2、Django、Vue.js 开发，Gerapy 可以帮助我们： 

更方便地控制爬虫运行
更直观地查看爬虫状态
更实时地查看爬取结果
更简单地实现项目部署
更统一地实现主机管理
```

安装

```
pip3 install gerapy
```



启动

```

新建文件夹
cd 文件夹

gerapy init

cd gerapy
gerapy migrate

gerapy runserver
```



主机管理

1. 添加scrapyd主机

   ![配置](images/gerapy_主机管理页面.png)![配置](images/gerapy_主机添加.png)

需要添加 IP、端口，以及名称，点击创建即可完成添加，点击返回即可看到当前添加的 Scrapyd 服务列表,创建成功后,我们可以在列表中查看已经添加的服务



 2.执行爬虫,就点击调度.然后运行. (**前提是: 我们配置的scrapyd中,已经发布了爬虫**.)

![配置](images/gerapy_列表.png)

![配置](images/gerapy_调度scrapy爬虫项目.png)





项目管理

>  cd deploy
>  scrapyd 



1.我们可以将scarpy项目直接放到 /gerapy/projects下. ![配置](images/project_1.png)



2.可以在gerapy后台看到有个项目 ![配置](images/project_list.png)



3.点击部署点击部署按钮进行打包和部署，在右下角我们可以输入打包时的描述信息，类似于 Git 的 commit 信息，然后点击打包按钮，即可发现 Gerapy 会提示打包成功，同时在左侧显示打包的结果和打包名称。 

![配置](images/project项目打包.png)![配置](images/build之后.png)





4.选择一个站点，点击右侧部署，将该项目部署到该站点上 ![配置](images/部署.png)



5.成功部署之后会显示描述和部署时间 ![配置](images/部署成功.png)



6.来到clients界面，找到部署该项目的节点，点击调度 ![配置](images/运行1.png)





7.在该节点中的项目列表中找到项目，点击右侧run运行项目 ![配置](images/运行2.png)











## day10

### 安装

> nox 夜神模拟器安装后记得将其添加到环境变量中



其他详细步骤参考视频老师课件



### 配置

> https://github.com/appium/appium/blob/master/docs/en/writing-running-appium/caps.md



1. 获取app包名和进程名
   - 清空后台
   - 打开夜神模拟器中的浏览器
   - 在adb连接正确的情况下，在夜神模拟器安装目录的bin目录下的cmd中输入`adb shell`
   - 进入adb shell后输入 `dumpsys activity | grep mFocusedActivity`
   - `com.android.browser`就是app包名
   - `.BrowserActivity`就是进程名
2. 启动Appium，点击start server
3. 点击放大镜进入并输入测试配置
   - platformName 系统名 `Android`
   - platformVersion 系统版本 `4.4.2`
   - deviceName 手机型号 `SM-G955F`
   - appPackage 应用的包名 `com.android.browser`
   - appActivity 应用的进程名 `.BrowserActivity`
4. 关闭夜神模拟器中的浏览器，点击appnium右下角的start session查看运行结果



### 控制设备并提取数据

adb指令

```
adb devices
nox_adb.exe connect 127.0.0.1:62001

adb shell 
# 在夜神市场中安装抖音
# 点击夜神模拟器中的抖音
dumpsys activity | grep mFocusedActivity
```



appnium 配置

```
    platformName: Android
    platformVersion: 4.4.2
    deviceName: SM-G955F
    appPackage: com.ss.android.ugc.aweme
    appActivity: .main.MainActivity
```



控制抖音完整代码

```python
import time
from appium import webdriver

# pip install appium-python-client
"""
1.安装并打开抖音，cmd输入 dumpsys activity | grep mFocusedActivity   获取到相关信息
2.

"""


class DouyinAction():
    """自动滑动，并获取抖音短视频发布者的id"""

    def __init__(self, nums: int = None):
        # 初始化配置，设置Desired Capabilities参数
        self.desired_caps = {
            'platformName': 'Android',
            'deviceName': 'SM-G955F',
            'appPackage': 'com.ss.android.ugc.aweme',
            'appActivity': '.main.MainActivity'
        }
        # 指定Appium Server
        self.server = 'http://localhost:4723/wd/hub'
        # 新建一个driver
        self.driver = webdriver.Remote(self.server, self.desired_caps)

        # 获取模拟器/手机的分辨率(px)
        width = self.driver.get_window_size()['width']
        height = self.driver.get_window_size()['height']
        print(width, height)
        # 设置滑动初始坐标和滑动距离
        self.start_x = width // 2  # 屏幕宽度中心点
        self.start_y = height // 3 * 2  # 屏幕高度从上开始到下三分之二处
        self.distance = height // 2  # 滑动距离：屏幕高度一半的距离
        # 设置滑动次数
        self.nums = nums

    def comments(self):
        # app开启之后点击一次屏幕，确保页面的展示
        time.sleep(2)
        self.driver.tap([(500, 1200)], 500)

    def scroll(self):

        print('滑动ing...')
        self.driver.swipe(self.start_x, self.start_y,
                          self.start_x, self.start_y - self.distance)
        time.sleep(3)
        # self.driver.find_element_by_xpath(
        #     '/hierarchy/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.HorizontalScrollView/android.widget.LinearLayout/android.widget.TabHost/android.widget.FrameLayout/android.widget.FrameLayout[2]/android.widget.LinearLayout/android.widget.FrameLayout[1]').click()
        self.driver.find_element_by_xpath(
    '/hierarchy/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.FrameLayout/android.widget.FrameLayout/android.widget.LinearLayout/android.widget.RelativeLayout/android.widget.LinearLayout/android.widget.LinearLayout/android.widget.TextView').click()
        time.sleep(3)
        # 无限滑动
        i = 0
        while True:
            # 模拟滑动
            print('滑动ing...')
            self.driver.swipe(self.start_x, self.start_y,
                              self.start_x, self.start_y - self.distance)
            time.sleep(3)
            self.get_infos()  # 获取视频发布者的名字
            # 设置延时等待
            time.sleep(4)
            # 判断是否退出
            if self.nums is not None and self.nums == i:
                break
            i += 1

    def get_infos(self):

        # 获取视频的各种信息：使用appium desktop定位元素
        # print(self.driver.find_element_by_id('ap').text)  # 发布者名字
        # print(self.driver.find_element_by_id('xm').text)  # 点赞数
        # print(self.driver.find_element_by_id('xn').text)  # 留言数
        # print(self.driver.find_element_by_id('oz').text)  # 视频名字，可能不存在，报错

        print(self.driver.find_element_by_id('a37').text)  # 点赞数
        print(self.driver.find_element_by_id('tm').text)  # 留言数

        # # 点击【分享】坐标位置 671,1058
        # self.driver.tap([(671, 1058)])
        # time.sleep(2)
        # # 向左滑动露出 【复制链接】 580，1100 --> 200, 1100
        # self.driver.swipe(580,1100, 20, 200, 1100)
        # # self.driver.get_screenshot_as_file('./a.png') # 截图
        # # 点击【复制链接】 距离右边60 距离底边170 720-60，1280-170
        # self.driver.tap([(660, 1110)])
        # # self.driver.get_screenshot_as_file('./b.png')  # 截图

    def main(self):
        self.comments()  # 点击一次屏幕，确保页面的展示
        time.sleep(2)
        self.scroll()  # 滑动


if __name__ == '__main__':
    action = DouyinAction(nums=5)
    action.main()

```

